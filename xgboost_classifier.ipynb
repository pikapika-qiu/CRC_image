{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/Desktop/project_data_new/embedding_768_TCGA_COAD.csv')\n",
    "data.index = data['PatientID']\n",
    "# drop last 2 columns\n",
    "data = data.drop(data.columns[-2:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PatientID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-A6-6650</th>\n",
       "      <td>-0.457220</td>\n",
       "      <td>0.851885</td>\n",
       "      <td>0.540813</td>\n",
       "      <td>-0.423622</td>\n",
       "      <td>0.714115</td>\n",
       "      <td>-0.564329</td>\n",
       "      <td>1.607602</td>\n",
       "      <td>0.543431</td>\n",
       "      <td>-0.870704</td>\n",
       "      <td>0.980508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673500</td>\n",
       "      <td>0.445102</td>\n",
       "      <td>0.272053</td>\n",
       "      <td>0.577840</td>\n",
       "      <td>-0.313309</td>\n",
       "      <td>0.463409</td>\n",
       "      <td>0.216033</td>\n",
       "      <td>1.875605</td>\n",
       "      <td>-0.811024</td>\n",
       "      <td>-1.250726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-A6-6142</th>\n",
       "      <td>-0.387709</td>\n",
       "      <td>0.730046</td>\n",
       "      <td>0.720710</td>\n",
       "      <td>-0.723050</td>\n",
       "      <td>0.841878</td>\n",
       "      <td>-0.463233</td>\n",
       "      <td>1.442049</td>\n",
       "      <td>0.573482</td>\n",
       "      <td>-0.817522</td>\n",
       "      <td>1.146394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287993</td>\n",
       "      <td>0.639623</td>\n",
       "      <td>0.308657</td>\n",
       "      <td>0.696497</td>\n",
       "      <td>-0.164076</td>\n",
       "      <td>0.249183</td>\n",
       "      <td>0.075877</td>\n",
       "      <td>2.179679</td>\n",
       "      <td>-0.814272</td>\n",
       "      <td>-1.311477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-5M-AAT4</th>\n",
       "      <td>-0.430112</td>\n",
       "      <td>0.766393</td>\n",
       "      <td>0.768542</td>\n",
       "      <td>-0.344121</td>\n",
       "      <td>0.560576</td>\n",
       "      <td>-0.597754</td>\n",
       "      <td>1.708495</td>\n",
       "      <td>0.419003</td>\n",
       "      <td>-0.934103</td>\n",
       "      <td>1.152953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578889</td>\n",
       "      <td>0.539531</td>\n",
       "      <td>0.399675</td>\n",
       "      <td>0.471158</td>\n",
       "      <td>-0.089670</td>\n",
       "      <td>0.701613</td>\n",
       "      <td>0.178599</td>\n",
       "      <td>1.802830</td>\n",
       "      <td>-0.844305</td>\n",
       "      <td>-1.300183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-A6-2676</th>\n",
       "      <td>-0.417006</td>\n",
       "      <td>0.964753</td>\n",
       "      <td>0.525541</td>\n",
       "      <td>-0.572356</td>\n",
       "      <td>0.777732</td>\n",
       "      <td>-0.462124</td>\n",
       "      <td>1.476457</td>\n",
       "      <td>0.640170</td>\n",
       "      <td>-0.722137</td>\n",
       "      <td>1.066126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475959</td>\n",
       "      <td>0.557490</td>\n",
       "      <td>0.094860</td>\n",
       "      <td>0.659872</td>\n",
       "      <td>-0.243587</td>\n",
       "      <td>0.122108</td>\n",
       "      <td>0.107965</td>\n",
       "      <td>2.146043</td>\n",
       "      <td>-0.946326</td>\n",
       "      <td>-1.345658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-A6-6652</th>\n",
       "      <td>-0.276354</td>\n",
       "      <td>0.766071</td>\n",
       "      <td>0.612194</td>\n",
       "      <td>-0.596442</td>\n",
       "      <td>0.814464</td>\n",
       "      <td>0.016801</td>\n",
       "      <td>1.598807</td>\n",
       "      <td>0.929635</td>\n",
       "      <td>-0.914753</td>\n",
       "      <td>1.071625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395806</td>\n",
       "      <td>0.545849</td>\n",
       "      <td>0.290805</td>\n",
       "      <td>0.229333</td>\n",
       "      <td>-0.536335</td>\n",
       "      <td>0.017757</td>\n",
       "      <td>-0.074384</td>\n",
       "      <td>2.026599</td>\n",
       "      <td>-0.776906</td>\n",
       "      <td>-0.934953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-G4-6321</th>\n",
       "      <td>-0.359843</td>\n",
       "      <td>0.929950</td>\n",
       "      <td>0.543955</td>\n",
       "      <td>-0.350527</td>\n",
       "      <td>0.404216</td>\n",
       "      <td>-0.566966</td>\n",
       "      <td>1.688820</td>\n",
       "      <td>0.196331</td>\n",
       "      <td>-1.050643</td>\n",
       "      <td>1.162059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802867</td>\n",
       "      <td>0.499934</td>\n",
       "      <td>0.404179</td>\n",
       "      <td>0.325667</td>\n",
       "      <td>-0.166769</td>\n",
       "      <td>0.612408</td>\n",
       "      <td>0.197617</td>\n",
       "      <td>1.686868</td>\n",
       "      <td>-0.917694</td>\n",
       "      <td>-1.251037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-RU-A8FL</th>\n",
       "      <td>-0.480253</td>\n",
       "      <td>0.931134</td>\n",
       "      <td>0.455165</td>\n",
       "      <td>-0.714430</td>\n",
       "      <td>0.685353</td>\n",
       "      <td>-0.373306</td>\n",
       "      <td>1.511218</td>\n",
       "      <td>0.747226</td>\n",
       "      <td>-0.652520</td>\n",
       "      <td>0.907837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179997</td>\n",
       "      <td>0.744851</td>\n",
       "      <td>0.062442</td>\n",
       "      <td>0.249245</td>\n",
       "      <td>-0.573766</td>\n",
       "      <td>-0.159084</td>\n",
       "      <td>-0.157464</td>\n",
       "      <td>2.173445</td>\n",
       "      <td>-0.916892</td>\n",
       "      <td>-1.098334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-G4-6303</th>\n",
       "      <td>-0.397405</td>\n",
       "      <td>0.793659</td>\n",
       "      <td>0.682602</td>\n",
       "      <td>-0.526544</td>\n",
       "      <td>0.733674</td>\n",
       "      <td>-0.456034</td>\n",
       "      <td>1.593480</td>\n",
       "      <td>0.500975</td>\n",
       "      <td>-0.982290</td>\n",
       "      <td>1.068642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514841</td>\n",
       "      <td>0.515760</td>\n",
       "      <td>0.324934</td>\n",
       "      <td>0.458768</td>\n",
       "      <td>-0.172873</td>\n",
       "      <td>0.498273</td>\n",
       "      <td>0.168238</td>\n",
       "      <td>1.823696</td>\n",
       "      <td>-0.805411</td>\n",
       "      <td>-1.238766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-QG-A5Z1</th>\n",
       "      <td>-0.422558</td>\n",
       "      <td>0.896717</td>\n",
       "      <td>0.599262</td>\n",
       "      <td>-0.347394</td>\n",
       "      <td>0.683292</td>\n",
       "      <td>-0.429929</td>\n",
       "      <td>1.659195</td>\n",
       "      <td>0.494966</td>\n",
       "      <td>-0.979689</td>\n",
       "      <td>1.184772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552484</td>\n",
       "      <td>0.555800</td>\n",
       "      <td>0.340869</td>\n",
       "      <td>0.324060</td>\n",
       "      <td>-0.257918</td>\n",
       "      <td>0.544318</td>\n",
       "      <td>0.087316</td>\n",
       "      <td>1.892267</td>\n",
       "      <td>-0.869365</td>\n",
       "      <td>-1.202163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-G4-6588</th>\n",
       "      <td>-0.395935</td>\n",
       "      <td>0.996501</td>\n",
       "      <td>0.597107</td>\n",
       "      <td>-0.325429</td>\n",
       "      <td>0.781881</td>\n",
       "      <td>-0.532040</td>\n",
       "      <td>1.566710</td>\n",
       "      <td>0.515049</td>\n",
       "      <td>-1.091114</td>\n",
       "      <td>1.071225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827478</td>\n",
       "      <td>0.501956</td>\n",
       "      <td>0.386623</td>\n",
       "      <td>0.433811</td>\n",
       "      <td>-0.190602</td>\n",
       "      <td>0.643315</td>\n",
       "      <td>0.131926</td>\n",
       "      <td>1.568284</td>\n",
       "      <td>-0.786084</td>\n",
       "      <td>-1.273287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0         1         2         3         4         5  \\\n",
       "PatientID                                                                  \n",
       "TCGA-A6-6650 -0.457220  0.851885  0.540813 -0.423622  0.714115 -0.564329   \n",
       "TCGA-A6-6142 -0.387709  0.730046  0.720710 -0.723050  0.841878 -0.463233   \n",
       "TCGA-5M-AAT4 -0.430112  0.766393  0.768542 -0.344121  0.560576 -0.597754   \n",
       "TCGA-A6-2676 -0.417006  0.964753  0.525541 -0.572356  0.777732 -0.462124   \n",
       "TCGA-A6-6652 -0.276354  0.766071  0.612194 -0.596442  0.814464  0.016801   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "TCGA-G4-6321 -0.359843  0.929950  0.543955 -0.350527  0.404216 -0.566966   \n",
       "TCGA-RU-A8FL -0.480253  0.931134  0.455165 -0.714430  0.685353 -0.373306   \n",
       "TCGA-G4-6303 -0.397405  0.793659  0.682602 -0.526544  0.733674 -0.456034   \n",
       "TCGA-QG-A5Z1 -0.422558  0.896717  0.599262 -0.347394  0.683292 -0.429929   \n",
       "TCGA-G4-6588 -0.395935  0.996501  0.597107 -0.325429  0.781881 -0.532040   \n",
       "\n",
       "                     6         7         8         9  ...       758       759  \\\n",
       "PatientID                                             ...                       \n",
       "TCGA-A6-6650  1.607602  0.543431 -0.870704  0.980508  ...  0.673500  0.445102   \n",
       "TCGA-A6-6142  1.442049  0.573482 -0.817522  1.146394  ...  0.287993  0.639623   \n",
       "TCGA-5M-AAT4  1.708495  0.419003 -0.934103  1.152953  ...  0.578889  0.539531   \n",
       "TCGA-A6-2676  1.476457  0.640170 -0.722137  1.066126  ...  0.475959  0.557490   \n",
       "TCGA-A6-6652  1.598807  0.929635 -0.914753  1.071625  ...  0.395806  0.545849   \n",
       "...                ...       ...       ...       ...  ...       ...       ...   \n",
       "TCGA-G4-6321  1.688820  0.196331 -1.050643  1.162059  ...  0.802867  0.499934   \n",
       "TCGA-RU-A8FL  1.511218  0.747226 -0.652520  0.907837  ...  0.179997  0.744851   \n",
       "TCGA-G4-6303  1.593480  0.500975 -0.982290  1.068642  ...  0.514841  0.515760   \n",
       "TCGA-QG-A5Z1  1.659195  0.494966 -0.979689  1.184772  ...  0.552484  0.555800   \n",
       "TCGA-G4-6588  1.566710  0.515049 -1.091114  1.071225  ...  0.827478  0.501956   \n",
       "\n",
       "                   760       761       762       763       764       765  \\\n",
       "PatientID                                                                  \n",
       "TCGA-A6-6650  0.272053  0.577840 -0.313309  0.463409  0.216033  1.875605   \n",
       "TCGA-A6-6142  0.308657  0.696497 -0.164076  0.249183  0.075877  2.179679   \n",
       "TCGA-5M-AAT4  0.399675  0.471158 -0.089670  0.701613  0.178599  1.802830   \n",
       "TCGA-A6-2676  0.094860  0.659872 -0.243587  0.122108  0.107965  2.146043   \n",
       "TCGA-A6-6652  0.290805  0.229333 -0.536335  0.017757 -0.074384  2.026599   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "TCGA-G4-6321  0.404179  0.325667 -0.166769  0.612408  0.197617  1.686868   \n",
       "TCGA-RU-A8FL  0.062442  0.249245 -0.573766 -0.159084 -0.157464  2.173445   \n",
       "TCGA-G4-6303  0.324934  0.458768 -0.172873  0.498273  0.168238  1.823696   \n",
       "TCGA-QG-A5Z1  0.340869  0.324060 -0.257918  0.544318  0.087316  1.892267   \n",
       "TCGA-G4-6588  0.386623  0.433811 -0.190602  0.643315  0.131926  1.568284   \n",
       "\n",
       "                   766       767  \n",
       "PatientID                         \n",
       "TCGA-A6-6650 -0.811024 -1.250726  \n",
       "TCGA-A6-6142 -0.814272 -1.311477  \n",
       "TCGA-5M-AAT4 -0.844305 -1.300183  \n",
       "TCGA-A6-2676 -0.946326 -1.345658  \n",
       "TCGA-A6-6652 -0.776906 -0.934953  \n",
       "...                ...       ...  \n",
       "TCGA-G4-6321 -0.917694 -1.251037  \n",
       "TCGA-RU-A8FL -0.916892 -1.098334  \n",
       "TCGA-G4-6303 -0.805411 -1.238766  \n",
       "TCGA-QG-A5Z1 -0.869365 -1.202163  \n",
       "TCGA-G4-6588 -0.786084 -1.273287  \n",
       "\n",
       "[460 rows x 768 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target = pd.read_csv('~/Desktop/project_data_new/target_768_avg_expanded.csv')\n",
    "data_target.index = data_target['Unnamed: 0']\n",
    "data_target = data_target.drop(['Unnamed: 0'], axis = 1)\n",
    "# only keep the columns with category in the name\n",
    "data_target = data_target.loc[:, data_target.columns.str.contains('category')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.index.isin(data_target.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFE(xgb, n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Choose top 100 features\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Fit and transform the data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m X_selected \u001b[38;5;241m=\u001b[39m rfe\u001b[38;5;241m.\u001b[39mfit_transform(X, Y_encoded)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_rfe.py:264\u001b[0m, in \u001b[0;36mRFE.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_rfe.py:311\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[0;32m--> 311\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m    314\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[1;32m    315\u001b[0m     estimator,\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[1;32m    317\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    318\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[0;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1532\u001b[0m     params,\n\u001b[1;32m   1533\u001b[0m     train_dmatrix,\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1535\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[1;32m   1536\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1537\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[1;32m   1538\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1539\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m   1540\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1541\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1542\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1543\u001b[0m )\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   2103\u001b[0m         )\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = data.values\n",
    "# Extract the target column for category_b_12\n",
    "Y = data_target['category_stromal_34'].values\n",
    "# Re-encode labels to start from zero\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "# Instantiate the XGBoost classifier\n",
    "xgb = XGBClassifier()\n",
    "# Use RFE with XGBoost as the estimator\n",
    "rfe = RFE(xgb, n_features_to_select=100)  # Choose top 100 features\n",
    "# Fit and transform the data\n",
    "X_selected = rfe.fit_transform(X, Y_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the selected features to csv \n",
    "np.savetxt(\"/home/qiuaodon/Desktop/CRC_image/Best_features_XGboost_results/Top_100_features_RFE_stromal_34.csv\", X_selected, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the selected features\n",
    "X_selected = pd.read_csv('/home/qiuaodon/Desktop/CRC_image/Best_features_XGboost_results/Top_100_features_RFE_stromal_34.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = data.values\n",
    "# Extract the target column for category_b_12\n",
    "Y = data_target['category_stromal_34'].values\n",
    "# Re-encode labels to start from zero\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "# Instantiate and fit XGBoost\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X, Y_encoded)\n",
    "\n",
    "# Get feature importances and select the top 100 indices\n",
    "importances = xgb.feature_importances_\n",
    "top_100_indices = np.argsort(importances)[-100:]\n",
    "\n",
    "# Select top 100 features\n",
    "X_selected = X[:, top_100_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# Extract the feature values from data\n",
    "X = data.values\n",
    "\n",
    "# Set the chunk size (number of target variables per part)\n",
    "chunk_size = 40\n",
    "\n",
    "# Loop through target columns in chunks\n",
    "for chunk_start in range(0, 241, chunk_size):\n",
    "    # Get the current chunk of target columns\n",
    "    chunk_end = min(chunk_start + chunk_size, 241)\n",
    "    target_chunk = data_target.columns[chunk_start:chunk_end]\n",
    "    \n",
    "    # Initialize list to store results for the current chunk\n",
    "    chunk_results = []\n",
    "\n",
    "    # Open a file to write classification reports for the current chunk\n",
    "    report_file_path = f\"/home/qiuaodon/Desktop/CRC_image/Best_features_XGboost_results/Classification_Reports_100features_XGB_part_{chunk_start}_{chunk_end}.txt\"\n",
    "    with open(report_file_path, \"w\") as report_file:\n",
    "        # Loop through each target column in the current chunk\n",
    "        for target_col in target_chunk:\n",
    "            # Extract the target column for the current category\n",
    "            Y = data_target[target_col].values\n",
    "\n",
    "            # Encode the labels if necessary\n",
    "            label_encoder = LabelEncoder()\n",
    "            Y = label_encoder.fit_transform(Y)\n",
    "            \n",
    "            # Select top 100 features using RFE\n",
    "            rfe = RFE(estimator=xgb_clf, n_features_to_select=100)  # Choose top 100 features\n",
    "            X_selected = rfe.fit_transform(X, Y)\n",
    "            \n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                X_selected, Y, test_size=0.2, random_state=42, stratify=Y\n",
    "            )\n",
    "            \n",
    "            # Define the hyperparameter distribution for RandomizedSearchCV\n",
    "            param_dist = {\n",
    "                # Most important hyperparameters\n",
    "                'max_depth': randint(6, 15),\n",
    "                'min_child_weight': randint(1, 10),\n",
    "                'subsample': uniform(0.9, 0.1),          # Higher is usually better\n",
    "                'colsample_bytree': uniform(0.9, 0.1),   # Range from 0.5 to 1.0\n",
    "                'learning_rate': uniform(0.01, 0.05),    # Starting from 0.01\n",
    "                # Regularization hyperparameters\n",
    "                'n_estimators': randint(800, 1500),\n",
    "                'gamma': uniform(2, 3),\n",
    "                # 'reg_alpha': uniform(0, 1),\n",
    "                # 'reg_lambda': uniform(1, 5),\n",
    "            }\n",
    "            \n",
    "            # Perform Randomized Search with cross-validation\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=xgb_clf,\n",
    "                param_distributions=param_dist,\n",
    "                n_iter=60, \n",
    "                cv=5,\n",
    "                scoring='f1_weighted',\n",
    "                n_jobs=-1,\n",
    "                verbose=1,\n",
    "                random_state=42\n",
    "            )\n",
    "            random_search.fit(X_train, Y_train)\n",
    "            \n",
    "            # Get the best model from Randomized Search\n",
    "            best_xgb = random_search.best_estimator_\n",
    "            best_params = random_search.best_params_\n",
    "            \n",
    "            # Fit the model on the training data for evaluation\n",
    "            best_xgb.fit(X_train, Y_train)\n",
    "            \n",
    "            # Evaluate on the training set\n",
    "            Y_train_pred = best_xgb.predict(X_train)\n",
    "            train_report_dict = classification_report(Y_train, Y_train_pred, output_dict=True)\n",
    "            train_report = classification_report(Y_train, Y_train_pred)\n",
    "            \n",
    "            # Evaluate on the test set\n",
    "            Y_test_pred = best_xgb.predict(X_test)\n",
    "            test_report_dict = classification_report(Y_test, Y_test_pred, output_dict=True)\n",
    "            test_report = classification_report(Y_test, Y_test_pred)\n",
    "            \n",
    "            # Write classification reports to the file\n",
    "            report_file.write(f\"Classification Report for {target_col} (Train Set):\\n\")\n",
    "            report_file.write(train_report)\n",
    "            report_file.write(\"\\n\")\n",
    "            report_file.write(f\"Classification Report for {target_col} (Test Set):\\n\")\n",
    "            report_file.write(test_report)\n",
    "            report_file.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            # Calculate and store test set metrics\n",
    "            test_precision = precision_score(Y_test, Y_test_pred, average='weighted')\n",
    "            test_recall = recall_score(Y_test, Y_test_pred, average='weighted')\n",
    "            test_accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "            \n",
    "            # Extract test set class-specific metrics for class '1'\n",
    "            test_class_1_metrics = test_report_dict.get('1', {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "            \n",
    "            # Calculate and store training set metrics\n",
    "            train_precision = precision_score(Y_train, Y_train_pred, average='weighted')\n",
    "            train_recall = recall_score(Y_train, Y_train_pred, average='weighted')\n",
    "            train_accuracy = accuracy_score(Y_train, Y_train_pred)\n",
    "            \n",
    "            # Extract training set class-specific metrics for class '1'\n",
    "            train_class_1_metrics = train_report_dict.get('1', {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "            \n",
    "            # Append results to chunk_results\n",
    "            chunk_results.append({\n",
    "                \"Target Variable\": target_col,\n",
    "                # Test set metrics\n",
    "                \"Test Precision\": test_precision,\n",
    "                \"Test Recall\": test_recall,\n",
    "                \"Test Accuracy\": test_accuracy,\n",
    "                \"Test Class 1 Precision\": test_class_1_metrics[\"precision\"],\n",
    "                \"Test Class 1 Recall\": test_class_1_metrics[\"recall\"],\n",
    "                \"Test Class 1 F1-Score\": test_class_1_metrics[\"f1-score\"],\n",
    "                # Training set metrics\n",
    "                \"Train Accuracy\": train_accuracy,\n",
    "                \"Train Class 1 F1-Score\": train_class_1_metrics[\"f1-score\"],\n",
    "                # Best hyperparameters\n",
    "                \"Best Hyperparameters\": best_params\n",
    "            })\n",
    "    \n",
    "    # Save the results for the current chunk to an Excel file\n",
    "    results_df = pd.DataFrame(chunk_results)\n",
    "    results_df.to_excel(\n",
    "        f\"/home/qiuaodon/Desktop/CRC_image/Best_features_XGboost_results/Precision_Recall_Accuracy_100features_XGB_part_{chunk_start}_{chunk_end}.xlsx\",\n",
    "        index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n",
      "Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.9385097728601925, 'gamma': 4.553410014550571, 'learning_rate': 0.025846100257813885, 'max_depth': 7, 'min_child_weight': 9, 'n_estimators': 1365, 'subsample': 0.9696029796674973}\n",
      "Cross-validation F1 scores on training set: [0.59825349 0.51407205 0.4841858  0.49874728 0.48648456]\n",
      "Average cross-validation F1 score on training set: 0.5163486353951173\n",
      "Classification Report for category_stromal_34 with 100 features (Train Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.61      0.66       120\n",
      "           1       0.61      0.66      0.63       119\n",
      "           2       0.68      0.74      0.71       120\n",
      "\n",
      "    accuracy                           0.67       359\n",
      "   macro avg       0.67      0.67      0.67       359\n",
      "weighted avg       0.67      0.67      0.67       359\n",
      "\n",
      "Classification Report for category_stromal_34 with 100 features (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.37      0.42        30\n",
      "           1       0.40      0.47      0.43        30\n",
      "           2       0.59      0.63      0.61        30\n",
      "\n",
      "    accuracy                           0.49        90\n",
      "   macro avg       0.49      0.49      0.49        90\n",
      "weighted avg       0.49      0.49      0.49        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "\n",
    "Y_category_b_12 = data_target['category_stromal_34'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_category_b_12 = label_encoder.fit_transform(Y_category_b_12)\n",
    "# Stratified split of the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_selected, Y_category_b_12, test_size=0.2, random_state=42, stratify=Y_category_b_12)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# Define the hyperparameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    # most important hyperparameters\n",
    "    'max_depth': randint(6, 15),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.9, 0.1),          # higher better\n",
    "    'colsample_bytree': uniform(0.9, 0.1),   # 0.5 to 1.0 higher better\n",
    "    'learning_rate': uniform(0.01, 0.05),     # 0.01\n",
    "    # regularization hyperparameters\n",
    "    'n_estimators': randint(1000, 1500),\n",
    "    'gamma': uniform(2, 3),\n",
    "    #'reg_alpha': uniform(0, 1),\n",
    "    #'reg_lambda': uniform(1, 5),             # Start from 1 to avoid zero regularization\n",
    "\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist, n_iter=150, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from Randomized Search:\", random_search.best_params_)\n",
    "\n",
    "# Cross-validation on the training set with the best model\n",
    "cv_scores = cross_val_score(best_xgb, X_train, Y_train, cv=5, scoring='f1_weighted')\n",
    "print(\"Cross-validation F1 scores on training set:\", cv_scores)\n",
    "print(\"Average cross-validation F1 score on training set:\", np.mean(cv_scores))\n",
    "\n",
    "# Fit the model on the training data for the final evaluation on test and train sets\n",
    "best_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_xgb.predict(X_train)\n",
    "print(\"Classification Report for category_stromal_34 with 100 features (Train Set):\")\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "Y_test_pred = best_xgb.predict(X_test)\n",
    "print(\"Classification Report for category_stromal_34 with 100 features (Test Set):\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n",
    "# Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.9655722635380742, 'gamma': 3.156189862896695, 'learning_rate': 0.04408068344863949, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 1416, 'subsample': 0.9496037454293407}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.9655722635380742, 'gamma': 3.156189862896695, 'learning_rate': 0.04408068344863949, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 1416, 'subsample': 0.9496037454293407}\n",
      "Cross-validation F1 scores on training set: [0.54285024 0.49941051 0.45932971 0.54278388 0.54638002]\n",
      "Average cross-validation F1 score on training set: 0.5181508722678907\n",
      "Classification Report for category_stromal_34 with 100 features (Train Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84       120\n",
      "           1       0.81      0.83      0.82       119\n",
      "           2       0.86      0.92      0.89       120\n",
      "\n",
      "    accuracy                           0.85       359\n",
      "   macro avg       0.85      0.85      0.85       359\n",
      "weighted avg       0.85      0.85      0.85       359\n",
      "\n",
      "Classification Report for category_stromal_34 with 100 features (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.47      0.53        30\n",
      "           1       0.47      0.50      0.48        30\n",
      "           2       0.66      0.77      0.71        30\n",
      "\n",
      "    accuracy                           0.58        90\n",
      "   macro avg       0.58      0.58      0.57        90\n",
      "weighted avg       0.58      0.58      0.57        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "\n",
    "Y_category_b_12 = data_target['category_stromal_34'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_category_b_12 = label_encoder.fit_transform(Y_category_b_12)\n",
    "# Stratified split of the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_selected, Y_category_b_12, test_size=0.2, random_state=42, stratify=Y_category_b_12)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# Define the hyperparameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    # most important hyperparameters\n",
    "    'max_depth': randint(6, 15),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.9, 0.1),          # higher better\n",
    "    'colsample_bytree': uniform(0.9, 0.1),   # 0.5 to 1.0 higher better\n",
    "    'learning_rate': uniform(0.01, 0.05),     # 0.01\n",
    "    # regularization hyperparameters\n",
    "    'n_estimators': randint(800, 1500),\n",
    "    'gamma': uniform(2, 3),\n",
    "    #'reg_alpha': uniform(0, 1),\n",
    "    #'reg_lambda': uniform(1, 5),             # Start from 1 to avoid zero regularization\n",
    "\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist, n_iter=60, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from Randomized Search:\", random_search.best_params_)\n",
    "\n",
    "# Cross-validation on the training set with the best model\n",
    "cv_scores = cross_val_score(best_xgb, X_train, Y_train, cv=5, scoring='f1_weighted')\n",
    "print(\"Cross-validation F1 scores on training set:\", cv_scores)\n",
    "print(\"Average cross-validation F1 score on training set:\", np.mean(cv_scores))\n",
    "\n",
    "# Fit the model on the training data for the final evaluation on test and train sets\n",
    "best_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_xgb.predict(X_train)\n",
    "print(\"Classification Report for category_stromal_34 with 100 features (Train Set):\")\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "Y_test_pred = best_xgb.predict(X_test)\n",
    "print(\"Classification Report for category_stromal_34 with 100 features (Test Set):\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n",
    "# Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.9655722635380742, 'gamma': 3.156189862896695, 'learning_rate': 0.04408068344863949, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 1416, 'subsample': 0.9496037454293407}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.9018360384495572, 'gamma': 0.9328502944301792, 'learning_rate': 0.09925589984899778, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 4419, 'reg_alpha': 0.3562978380769749, 'reg_lambda': 5.53414220772877, 'subsample': 0.6360661246923176}\n",
      "Cross-validation F1 scores on training set: [0.46427662 0.38986928 0.5047138  0.42394934 0.51685606]\n",
      "Average cross-validation F1 score on training set: 0.45993302140403014\n",
      "Classification Report for category_b_12 with 100 features (Train Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       120\n",
      "           1       1.00      1.00      1.00       119\n",
      "           2       1.00      1.00      1.00       120\n",
      "\n",
      "    accuracy                           1.00       359\n",
      "   macro avg       1.00      1.00      1.00       359\n",
      "weighted avg       1.00      1.00      1.00       359\n",
      "\n",
      "Classification Report for category_b_12 with 100 features (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.50      0.53        30\n",
      "           1       0.47      0.50      0.48        30\n",
      "           2       0.61      0.63      0.62        30\n",
      "\n",
      "    accuracy                           0.54        90\n",
      "   macro avg       0.55      0.54      0.54        90\n",
      "weighted avg       0.55      0.54      0.54        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Assuming X and Y_category_b_12 are already defined\n",
    "X = data.values\n",
    "Y_category_b_12 = data_target['category_b_12'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_category_b_12 = label_encoder.fit_transform(Y_category_b_12)\n",
    "# Stratified split of the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_category_b_12, test_size=0.2, random_state=42, stratify=Y_category_b_12)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# Define the hyperparameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': randint(4000, 5000),\n",
    "    'learning_rate': uniform(0.01, 0.1),     # 0.01 to 0.11\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.5, 0.5),          # 0.5 to 1.0\n",
    "    'colsample_bytree': uniform(0.5, 0.5),   # 0.5 to 1.0\n",
    "    'gamma': uniform(0, 5),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(1, 5),             # Start from 1 to avoid zero regularization\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist, n_iter=50, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from Randomized Search:\", random_search.best_params_)\n",
    "\n",
    "# Cross-validation on the training set with the best model\n",
    "cv_scores = cross_val_score(best_xgb, X_train, Y_train, cv=5, scoring='f1_weighted')\n",
    "print(\"Cross-validation F1 scores on training set:\", cv_scores)\n",
    "print(\"Average cross-validation F1 score on training set:\", np.mean(cv_scores))\n",
    "\n",
    "# Fit the model on the training data for the final evaluation on test and train sets\n",
    "best_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_xgb.predict(X_train)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Train Set):\")\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "Y_test_pred = best_xgb.predict(X_test)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Test Set):\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.7323592099410596, 'gamma': 0.03177917514301182, 'learning_rate': 0.10329469651469865, 'max_depth': 10, 'min_child_weight': 5, 'n_estimators': 198, 'subsample': 0.8365191150830908}\n",
      "Cross-validation F1 scores on training set: [0.50189167 0.41718132 0.49907191 0.39585521 0.52024264]\n",
      "Average cross-validation F1 score on training set: 0.466848549262388\n",
      "Classification Report for category_b_12 with 100 features (Train Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       120\n",
      "           1       1.00      1.00      1.00       119\n",
      "           2       1.00      1.00      1.00       120\n",
      "\n",
      "    accuracy                           1.00       359\n",
      "   macro avg       1.00      1.00      1.00       359\n",
      "weighted avg       1.00      1.00      1.00       359\n",
      "\n",
      "Classification Report for category_b_12 with 100 features (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.47      0.47        30\n",
      "           1       0.45      0.43      0.44        30\n",
      "           2       0.55      0.57      0.56        30\n",
      "\n",
      "    accuracy                           0.49        90\n",
      "   macro avg       0.49      0.49      0.49        90\n",
      "weighted avg       0.49      0.49      0.49        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Assuming X and Y_category_b_12 are already defined\n",
    "X = data.values\n",
    "Y_category_b_12 = data_target['category_b_12'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_category_b_12 = label_encoder.fit_transform(Y_category_b_12)\n",
    "# Stratified split of the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_category_b_12, test_size=0.2, random_state=42, stratify=Y_category_b_12)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# Define the hyperparameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'gamma': uniform(0, 0.5)\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist, n_iter=50, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from Randomized Search:\", random_search.best_params_)\n",
    "\n",
    "# Cross-validation on the training set with the best model\n",
    "cv_scores = cross_val_score(best_xgb, X_train, Y_train, cv=5, scoring='f1_weighted')\n",
    "print(\"Cross-validation F1 scores on training set:\", cv_scores)\n",
    "print(\"Average cross-validation F1 score on training set:\", np.mean(cv_scores))\n",
    "\n",
    "# Fit the model on the training data for the final evaluation on test and train sets\n",
    "best_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_xgb.predict(X_train)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Train Set):\")\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "Y_test_pred = best_xgb.predict(X_test)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Test Set):\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.5257393756249946, 'gamma': 1.3932323211830573, 'learning_rate': 0.09174392973699882, 'max_depth': 4, 'min_child_weight': 6, 'n_estimators': 552, 'reg_alpha': 0.489452760277563, 'reg_lambda': 9.856504541106007, 'subsample': 0.6210276357557503}\n",
      "Cross-validation F1 scores on training set: [0.48863636 0.38843055 0.47293886 0.36705517 0.54819698]\n",
      "Average cross-validation F1 score on training set: 0.45305158492273556\n",
      "Classification Report for category_b_12 with 100 features (Train Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       120\n",
      "           1       0.98      0.97      0.98       119\n",
      "           2       0.98      0.98      0.98       120\n",
      "\n",
      "    accuracy                           0.98       359\n",
      "   macro avg       0.98      0.98      0.98       359\n",
      "weighted avg       0.98      0.98      0.98       359\n",
      "\n",
      "Classification Report for category_b_12 with 100 features (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.50      0.52        30\n",
      "           1       0.45      0.43      0.44        30\n",
      "           2       0.58      0.63      0.60        30\n",
      "\n",
      "    accuracy                           0.52        90\n",
      "   macro avg       0.52      0.52      0.52        90\n",
      "weighted avg       0.52      0.52      0.52        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Assuming X and Y_category_b_12 are already defined\n",
    "X = data.values\n",
    "Y_category_b_12 = data_target['category_b_12'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_category_b_12 = label_encoder.fit_transform(Y_category_b_12)\n",
    "# Stratified split of the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_category_b_12, test_size=0.2, random_state=42, stratify=Y_category_b_12)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(500, 1000),  # Increased upper bound\n",
    "    'max_depth': randint(2, 8),          # Reduced depth\n",
    "    'learning_rate': uniform(0.01, 0.09),# Lower learning rates\n",
    "    'subsample': uniform(0.5, 0.5),      # Range from 0.5 to 1.0\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'min_child_weight': randint(5, 15),  # Increased minimum\n",
    "    'gamma': uniform(0, 5),              # Expanded range\n",
    "    'reg_alpha': uniform(0, 1),          # Added L1 regularization\n",
    "    'reg_lambda': uniform(0, 10),        # Added L2 regularization\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist, n_iter=50, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from Randomized Search:\", random_search.best_params_)\n",
    "\n",
    "# Cross-validation on the training set with the best model\n",
    "cv_scores = cross_val_score(best_xgb, X_train, Y_train, cv=5, scoring='f1_weighted')\n",
    "print(\"Cross-validation F1 scores on training set:\", cv_scores)\n",
    "print(\"Average cross-validation F1 score on training set:\", np.mean(cv_scores))\n",
    "\n",
    "# Fit the model on the training data for the final evaluation on test and train sets\n",
    "best_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_xgb.predict(X_train)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Train Set):\")\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "Y_test_pred = best_xgb.predict(X_test)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Test Set):\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.5866471600354228, 'gamma': 0.7821852133554302, 'learning_rate': 0.03252186083481358, 'max_depth': 7, 'min_child_weight': 10, 'n_estimators': 547, 'reg_alpha': 0.18286599710730733, 'reg_lambda': 9.346139973397097, 'subsample': 0.8191352969216752}\n",
      "Cross-validation F1 scores on training set: [0.56608155 0.44555012 0.51604938 0.51551307 0.51535495]\n",
      "Average cross-validation F1 score on training set: 0.5117098133827519\n",
      "Classification Report for category_b_12 with 100 selected features (Train Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       120\n",
      "           1       0.98      0.97      0.98       119\n",
      "           2       0.98      0.98      0.98       120\n",
      "\n",
      "    accuracy                           0.98       359\n",
      "   macro avg       0.98      0.98      0.98       359\n",
      "weighted avg       0.98      0.98      0.98       359\n",
      "\n",
      "Classification Report for category_b_12 with 100 selected features (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.40      0.39        30\n",
      "           1       0.48      0.47      0.47        30\n",
      "           2       0.69      0.67      0.68        30\n",
      "\n",
      "    accuracy                           0.51        90\n",
      "   macro avg       0.52      0.51      0.51        90\n",
      "weighted avg       0.52      0.51      0.51        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Assuming X and Y_category_b_12 are already defined\n",
    "X = data.values\n",
    "Y_category_b_12 = data_target['category_b_12'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_category_b_12 = label_encoder.fit_transform(Y_category_b_12)\n",
    "\n",
    "# Stratified split of the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_category_b_12, test_size=0.2, random_state=42, stratify=Y_category_b_12)\n",
    "\n",
    "# Step 1: Initialize XGBoost Classifier and RFE\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "rfe = RFE(estimator=xgb_clf, n_features_to_select=100)  # Choose top 100 features\n",
    "\n",
    "# Fit RFE to select top 100 features\n",
    "X_train_rfe = rfe.fit_transform(X_train, Y_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(500, 1000),  # Increased upper bound\n",
    "    'max_depth': randint(2, 8),          # Reduced depth\n",
    "    'learning_rate': uniform(0.01, 0.09),# Lower learning rates\n",
    "    'subsample': uniform(0.5, 0.5),      # Range from 0.5 to 1.0\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'min_child_weight': randint(5, 15),  # Increased minimum\n",
    "    'gamma': uniform(0, 5),              # Expanded range\n",
    "    'reg_alpha': uniform(0, 1),          # Added L1 regularization\n",
    "    'reg_lambda': uniform(0, 10),        # Added L2 regularization\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation on the selected features\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist, n_iter=50, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train_rfe, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from Randomized Search:\", random_search.best_params_)\n",
    "\n",
    "# Cross-validation on the training set with the best model\n",
    "cv_scores = cross_val_score(best_xgb, X_train_rfe, Y_train, cv=5, scoring='f1_weighted')\n",
    "print(\"Cross-validation F1 scores on training set:\", cv_scores)\n",
    "print(\"Average cross-validation F1 score on training set:\", np.mean(cv_scores))\n",
    "\n",
    "# Step 3: Fit the model on the training data for the final evaluation on test and train sets\n",
    "best_xgb.fit(X_train_rfe, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_xgb.predict(X_train_rfe)\n",
    "print(\"Classification Report for category_b_12 with 100 selected features (Train Set):\")\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "Y_test_pred = best_xgb.predict(X_test_rfe)\n",
    "print(\"Classification Report for category_b_12 with 100 selected features (Test Set):\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Assuming 'data' and 'data_target' are your feature matrix and target DataFrame respectively\n",
    "# X contains feature values\n",
    "X = data.values\n",
    "\n",
    "# Set the chunk size (number of target variables per part)\n",
    "chunk_size = 40\n",
    "\n",
    "# Loop through target columns in chunks\n",
    "for chunk_start in range(0, 241, chunk_size):\n",
    "    # Get the current chunk of target columns\n",
    "    chunk_end = min(chunk_start + chunk_size, 241)\n",
    "    target_chunk = data_target.columns[chunk_start:chunk_end]\n",
    "    \n",
    "    # Initialize list to store results for the current chunk\n",
    "    chunk_results = []\n",
    "\n",
    "    # Open a file to write classification reports for the current chunk\n",
    "    report_file_path = f\"/home/qiuaodon/Desktop/CRC_image/XGboost_results/Classification_Reports_100features_part_{chunk_start}_{chunk_end}.txt\"\n",
    "    with open(report_file_path, \"w\") as report_file:\n",
    "        # Iterate over each target column in the chunk\n",
    "        for target_col in target_chunk:\n",
    "            # Extract the target column for the current category\n",
    "            Y = data_target[target_col].values\n",
    "\n",
    "            # Encode labels if they are not numerical\n",
    "            label_encoder = LabelEncoder()\n",
    "            Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "            # Initialize the XGBoost Classifier\n",
    "            xgb_clf = xgb.XGBClassifier(\n",
    "                objective='multi:softmax', \n",
    "                random_state=42, \n",
    "                use_label_encoder=False,\n",
    "                eval_metric='mlogloss'\n",
    "            )\n",
    "\n",
    "            # Select top 100 features using RFE\n",
    "            rfe = RFE(estimator=xgb_clf, n_features_to_select=100)\n",
    "            X_selected = rfe.fit_transform(X, Y_encoded)\n",
    "            \n",
    "            # Split the data into training and testing sets with stratification\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                X_selected, Y_encoded, test_size=0.2, random_state=42, stratify=Y_encoded\n",
    "            )\n",
    "\n",
    "            # Define the hyperparameter distribution for XGBoost\n",
    "            param_dist = {\n",
    "                'n_estimators': randint(500, 1000),  # Increased upper bound\n",
    "                'max_depth': randint(2, 8),          # Reduced depth\n",
    "                'learning_rate': uniform(0.01, 0.09),# Lower learning rates\n",
    "                'subsample': uniform(0.5, 0.5),      # Range from 0.5 to 1.0\n",
    "                'colsample_bytree': uniform(0.5, 0.5),\n",
    "                'min_child_weight': randint(5, 15),  # Increased minimum\n",
    "                'gamma': uniform(0, 5),              # Expanded range\n",
    "                'reg_alpha': uniform(0, 1),          # Added L1 regularization\n",
    "                'reg_lambda': uniform(0, 10),        # Added L2 regularization\n",
    "            }\n",
    "\n",
    "            # Perform Randomized Search with cross-validation\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=xgb_clf,\n",
    "                param_distributions=param_dist,\n",
    "                n_iter=50, \n",
    "                cv=5, \n",
    "                scoring='f1_weighted', \n",
    "                n_jobs=-1, \n",
    "                verbose=1, \n",
    "                random_state=42\n",
    "            )\n",
    "            random_search.fit(X_train, Y_train)\n",
    "\n",
    "            # Get the best model from Randomized Search\n",
    "            best_xgb = random_search.best_estimator_\n",
    "            best_params = random_search.best_params_\n",
    "\n",
    "            # Fit the best model on the training data\n",
    "            best_xgb.fit(X_train, Y_train)\n",
    "\n",
    "            # Evaluate on the training set\n",
    "            Y_train_pred = best_xgb.predict(X_train)\n",
    "            train_report = classification_report(Y_train, Y_train_pred)\n",
    "\n",
    "            # Evaluate on the test set\n",
    "            Y_test_pred = best_xgb.predict(X_test)\n",
    "            test_report_dict = classification_report(Y_test, Y_test_pred, output_dict=True)\n",
    "            test_report = classification_report(Y_test, Y_test_pred)\n",
    "\n",
    "            # Write classification reports to the file\n",
    "            report_file.write(f\"Classification Report for {target_col} (Train Set):\\n\")\n",
    "            report_file.write(train_report)\n",
    "            report_file.write(\"\\n\")\n",
    "            report_file.write(f\"Classification Report for {target_col} (Test Set):\\n\")\n",
    "            report_file.write(test_report)\n",
    "            report_file.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "            # Calculate and store precision, recall, and accuracy\n",
    "            precision = precision_score(Y_test, Y_test_pred, average='weighted')\n",
    "            recall = recall_score(Y_test, Y_test_pred, average='weighted')\n",
    "            accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "\n",
    "            # Extract precision, recall, and f1-score specifically for class '1' (if it exists)\n",
    "            class_labels = label_encoder.classes_\n",
    "            class_indices = label_encoder.transform(class_labels)\n",
    "            class_1_index = np.where(class_indices == 1)[0]\n",
    "\n",
    "            if class_1_index.size > 0:\n",
    "                class_1_label = str(class_indices[class_1_index[0]])\n",
    "                class_1_metrics = test_report_dict.get(class_1_label, {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "            else:\n",
    "                class_1_metrics = {\"precision\": None, \"recall\": None, \"f1-score\": None}\n",
    "\n",
    "            chunk_results.append({\n",
    "                \"Target Variable\": target_col,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Class 1 Precision\": class_1_metrics[\"precision\"],\n",
    "                \"Class 1 Recall\": class_1_metrics[\"recall\"],\n",
    "                \"Class 1 F1-Score\": class_1_metrics[\"f1-score\"],\n",
    "                \"Best Hyperparameters\": best_params\n",
    "            })\n",
    "    \n",
    "    # Save the results for the current chunk to an Excel file\n",
    "    results_df = pd.DataFrame(chunk_results)\n",
    "    results_df.to_excel(\n",
    "        f\"/home/qiuaodon/Desktop/CRC_image/XGboost_results/Precision_Recall_Accuracy_100features_part_{chunk_start}_{chunk_end}.xlsx\",\n",
    "        index=False\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
