{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/Desktop/project_data_new/embedding_768_TCGA_COAD.csv')\n",
    "data.index = data['PatientID']\n",
    "# drop last 2 columns\n",
    "data = data.drop(data.columns[-2:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target = pd.read_csv('~/Desktop/project_data_new/target_768_avg_expanded.csv')\n",
    "data_target.index = data_target['Unnamed: 0']\n",
    "data_target = data_target.drop(['Unnamed: 0'], axis = 1)\n",
    "# only keep the columns with category in the name\n",
    "data_target = data_target.loc[:, data_target.columns.str.contains('category')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.index.isin(data_target.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFE(xgb, n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Choose top 100 features\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Fit and transform the data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m X_selected \u001b[38;5;241m=\u001b[39m rfe\u001b[38;5;241m.\u001b[39mfit_transform(X, Y_encoded)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_rfe.py:264\u001b[0m, in \u001b[0;36mRFE.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_rfe.py:311\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[0;32m--> 311\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m    314\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[1;32m    315\u001b[0m     estimator,\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[1;32m    317\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    318\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[0;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1532\u001b[0m     params,\n\u001b[1;32m   1533\u001b[0m     train_dmatrix,\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1535\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[1;32m   1536\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1537\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[1;32m   1538\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1539\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m   1540\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1541\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1542\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1543\u001b[0m )\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   2103\u001b[0m         )\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = data.values\n",
    "# Extract the target column for category_b_12\n",
    "Y = data_target['category_stromal_34'].values\n",
    "# Re-encode labels to start from zero\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "# Instantiate the XGBoost classifier\n",
    "xgb = XGBClassifier()\n",
    "# Use RFE with XGBoost as the estimator\n",
    "rfe = RFE(xgb, n_features_to_select=100)  # Choose top 100 features\n",
    "# Fit and transform the data\n",
    "X_selected = rfe.fit_transform(X, Y_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the selected features to csv \n",
    "np.savetxt(\"/home/qiuaodon/Desktop/CRC_image/Best_features_XGboost_results/Top_100_features_RFE_stromal_34.csv\", X_selected, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the selected features\n",
    "X_selected = pd.read_csv('/home/qiuaodon/Desktop/CRC_image/Best_features_XGboost_results/Top_100_features_RFE_stromal_34.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.9655722635380742, 'gamma': 3.156189862896695, 'learning_rate': 0.04408068344863949, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 1416, 'subsample': 0.9496037454293407}\n",
      "Cross-validation F1 scores on training set: [0.54285024 0.49941051 0.45932971 0.54278388 0.54638002]\n",
      "Average cross-validation F1 score on training set: 0.5181508722678907\n",
      "Classification Report for category_stromal_34 with 100 features (Train Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84       120\n",
      "           1       0.81      0.83      0.82       119\n",
      "           2       0.86      0.92      0.89       120\n",
      "\n",
      "    accuracy                           0.85       359\n",
      "   macro avg       0.85      0.85      0.85       359\n",
      "weighted avg       0.85      0.85      0.85       359\n",
      "\n",
      "Classification Report for category_stromal_34 with 100 features (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.47      0.53        30\n",
      "           1       0.47      0.50      0.48        30\n",
      "           2       0.66      0.77      0.71        30\n",
      "\n",
      "    accuracy                           0.58        90\n",
      "   macro avg       0.58      0.58      0.57        90\n",
      "weighted avg       0.58      0.58      0.57        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "\n",
    "Y_category_b_12 = data_target['category_stromal_34'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_category_b_12 = label_encoder.fit_transform(Y_category_b_12)\n",
    "# Stratified split of the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_selected, Y_category_b_12, test_size=0.2, random_state=42, stratify=Y_category_b_12)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# Define the hyperparameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    # most important hyperparameters\n",
    "    'max_depth': randint(6, 15),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.9, 0.1),          # higher better\n",
    "    'colsample_bytree': uniform(0.9, 0.1),   # 0.5 to 1.0 higher better\n",
    "    'learning_rate': uniform(0.01, 0.05),     # 0.01\n",
    "    # regularization hyperparameters\n",
    "    'n_estimators': randint(800, 1500),\n",
    "    'gamma': uniform(2, 3),\n",
    "    #'reg_alpha': uniform(0, 1),\n",
    "    #'reg_lambda': uniform(1, 5),             # Start from 1 to avoid zero regularization\n",
    "\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist, n_iter=60, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from Randomized Search:\", random_search.best_params_)\n",
    "\n",
    "# Cross-validation on the training set with the best model\n",
    "cv_scores = cross_val_score(best_xgb, X_train, Y_train, cv=5, scoring='f1_weighted')\n",
    "print(\"Cross-validation F1 scores on training set:\", cv_scores)\n",
    "print(\"Average cross-validation F1 score on training set:\", np.mean(cv_scores))\n",
    "\n",
    "# Fit the model on the training data for the final evaluation on test and train sets\n",
    "best_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_xgb.predict(X_train)\n",
    "print(\"Classification Report for category_stromal_34 with 100 features (Train Set):\")\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "Y_test_pred = best_xgb.predict(X_test)\n",
    "print(\"Classification Report for category_stromal_34 with 100 features (Test Set):\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n",
    "# Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.9655722635380742, 'gamma': 3.156189862896695, 'learning_rate': 0.04408068344863949, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 1416, 'subsample': 0.9496037454293407}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.9018360384495572, 'gamma': 0.9328502944301792, 'learning_rate': 0.09925589984899778, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 4419, 'reg_alpha': 0.3562978380769749, 'reg_lambda': 5.53414220772877, 'subsample': 0.6360661246923176}\n",
      "Cross-validation F1 scores on training set: [0.46427662 0.38986928 0.5047138  0.42394934 0.51685606]\n",
      "Average cross-validation F1 score on training set: 0.45993302140403014\n",
      "Classification Report for category_b_12 with 100 features (Train Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       120\n",
      "           1       1.00      1.00      1.00       119\n",
      "           2       1.00      1.00      1.00       120\n",
      "\n",
      "    accuracy                           1.00       359\n",
      "   macro avg       1.00      1.00      1.00       359\n",
      "weighted avg       1.00      1.00      1.00       359\n",
      "\n",
      "Classification Report for category_b_12 with 100 features (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.50      0.53        30\n",
      "           1       0.47      0.50      0.48        30\n",
      "           2       0.61      0.63      0.62        30\n",
      "\n",
      "    accuracy                           0.54        90\n",
      "   macro avg       0.55      0.54      0.54        90\n",
      "weighted avg       0.55      0.54      0.54        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Assuming X and Y_category_b_12 are already defined\n",
    "X = data.values\n",
    "Y_category_b_12 = data_target['category_b_12'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_category_b_12 = label_encoder.fit_transform(Y_category_b_12)\n",
    "# Stratified split of the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_category_b_12, test_size=0.2, random_state=42, stratify=Y_category_b_12)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# Define the hyperparameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': randint(4000, 5000),\n",
    "    'learning_rate': uniform(0.01, 0.1),     # 0.01 to 0.11\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.5, 0.5),          # 0.5 to 1.0\n",
    "    'colsample_bytree': uniform(0.5, 0.5),   # 0.5 to 1.0\n",
    "    'gamma': uniform(0, 5),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(1, 5),             # Start from 1 to avoid zero regularization\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist, n_iter=50, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from Randomized Search:\", random_search.best_params_)\n",
    "\n",
    "# Cross-validation on the training set with the best model\n",
    "cv_scores = cross_val_score(best_xgb, X_train, Y_train, cv=5, scoring='f1_weighted')\n",
    "print(\"Cross-validation F1 scores on training set:\", cv_scores)\n",
    "print(\"Average cross-validation F1 score on training set:\", np.mean(cv_scores))\n",
    "\n",
    "# Fit the model on the training data for the final evaluation on test and train sets\n",
    "best_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_xgb.predict(X_train)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Train Set):\")\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "Y_test_pred = best_xgb.predict(X_test)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Test Set):\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.7323592099410596, 'gamma': 0.03177917514301182, 'learning_rate': 0.10329469651469865, 'max_depth': 10, 'min_child_weight': 5, 'n_estimators': 198, 'subsample': 0.8365191150830908}\n",
      "Cross-validation F1 scores on training set: [0.50189167 0.41718132 0.49907191 0.39585521 0.52024264]\n",
      "Average cross-validation F1 score on training set: 0.466848549262388\n",
      "Classification Report for category_b_12 with 100 features (Train Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       120\n",
      "           1       1.00      1.00      1.00       119\n",
      "           2       1.00      1.00      1.00       120\n",
      "\n",
      "    accuracy                           1.00       359\n",
      "   macro avg       1.00      1.00      1.00       359\n",
      "weighted avg       1.00      1.00      1.00       359\n",
      "\n",
      "Classification Report for category_b_12 with 100 features (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.47      0.47        30\n",
      "           1       0.45      0.43      0.44        30\n",
      "           2       0.55      0.57      0.56        30\n",
      "\n",
      "    accuracy                           0.49        90\n",
      "   macro avg       0.49      0.49      0.49        90\n",
      "weighted avg       0.49      0.49      0.49        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Assuming X and Y_category_b_12 are already defined\n",
    "X = data.values\n",
    "Y_category_b_12 = data_target['category_b_12'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_category_b_12 = label_encoder.fit_transform(Y_category_b_12)\n",
    "# Stratified split of the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_category_b_12, test_size=0.2, random_state=42, stratify=Y_category_b_12)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# Define the hyperparameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'gamma': uniform(0, 0.5)\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist, n_iter=50, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from Randomized Search:\", random_search.best_params_)\n",
    "\n",
    "# Cross-validation on the training set with the best model\n",
    "cv_scores = cross_val_score(best_xgb, X_train, Y_train, cv=5, scoring='f1_weighted')\n",
    "print(\"Cross-validation F1 scores on training set:\", cv_scores)\n",
    "print(\"Average cross-validation F1 score on training set:\", np.mean(cv_scores))\n",
    "\n",
    "# Fit the model on the training data for the final evaluation on test and train sets\n",
    "best_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_xgb.predict(X_train)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Train Set):\")\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "Y_test_pred = best_xgb.predict(X_test)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Test Set):\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.5257393756249946, 'gamma': 1.3932323211830573, 'learning_rate': 0.09174392973699882, 'max_depth': 4, 'min_child_weight': 6, 'n_estimators': 552, 'reg_alpha': 0.489452760277563, 'reg_lambda': 9.856504541106007, 'subsample': 0.6210276357557503}\n",
      "Cross-validation F1 scores on training set: [0.48863636 0.38843055 0.47293886 0.36705517 0.54819698]\n",
      "Average cross-validation F1 score on training set: 0.45305158492273556\n",
      "Classification Report for category_b_12 with 100 features (Train Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       120\n",
      "           1       0.98      0.97      0.98       119\n",
      "           2       0.98      0.98      0.98       120\n",
      "\n",
      "    accuracy                           0.98       359\n",
      "   macro avg       0.98      0.98      0.98       359\n",
      "weighted avg       0.98      0.98      0.98       359\n",
      "\n",
      "Classification Report for category_b_12 with 100 features (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.50      0.52        30\n",
      "           1       0.45      0.43      0.44        30\n",
      "           2       0.58      0.63      0.60        30\n",
      "\n",
      "    accuracy                           0.52        90\n",
      "   macro avg       0.52      0.52      0.52        90\n",
      "weighted avg       0.52      0.52      0.52        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Assuming X and Y_category_b_12 are already defined\n",
    "X = data.values\n",
    "Y_category_b_12 = data_target['category_b_12'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_category_b_12 = label_encoder.fit_transform(Y_category_b_12)\n",
    "# Stratified split of the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_category_b_12, test_size=0.2, random_state=42, stratify=Y_category_b_12)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(500, 1000),  # Increased upper bound\n",
    "    'max_depth': randint(2, 8),          # Reduced depth\n",
    "    'learning_rate': uniform(0.01, 0.09),# Lower learning rates\n",
    "    'subsample': uniform(0.5, 0.5),      # Range from 0.5 to 1.0\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'min_child_weight': randint(5, 15),  # Increased minimum\n",
    "    'gamma': uniform(0, 5),              # Expanded range\n",
    "    'reg_alpha': uniform(0, 1),          # Added L1 regularization\n",
    "    'reg_lambda': uniform(0, 10),        # Added L2 regularization\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist, n_iter=50, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from Randomized Search:\", random_search.best_params_)\n",
    "\n",
    "# Cross-validation on the training set with the best model\n",
    "cv_scores = cross_val_score(best_xgb, X_train, Y_train, cv=5, scoring='f1_weighted')\n",
    "print(\"Cross-validation F1 scores on training set:\", cv_scores)\n",
    "print(\"Average cross-validation F1 score on training set:\", np.mean(cv_scores))\n",
    "\n",
    "# Fit the model on the training data for the final evaluation on test and train sets\n",
    "best_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_xgb.predict(X_train)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Train Set):\")\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "Y_test_pred = best_xgb.predict(X_test)\n",
    "print(\"Classification Report for category_b_12 with 100 features (Test Set):\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Hyperparameters from Randomized Search: {'colsample_bytree': 0.5866471600354228, 'gamma': 0.7821852133554302, 'learning_rate': 0.03252186083481358, 'max_depth': 7, 'min_child_weight': 10, 'n_estimators': 547, 'reg_alpha': 0.18286599710730733, 'reg_lambda': 9.346139973397097, 'subsample': 0.8191352969216752}\n",
      "Cross-validation F1 scores on training set: [0.56608155 0.44555012 0.51604938 0.51551307 0.51535495]\n",
      "Average cross-validation F1 score on training set: 0.5117098133827519\n",
      "Classification Report for category_b_12 with 100 selected features (Train Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       120\n",
      "           1       0.98      0.97      0.98       119\n",
      "           2       0.98      0.98      0.98       120\n",
      "\n",
      "    accuracy                           0.98       359\n",
      "   macro avg       0.98      0.98      0.98       359\n",
      "weighted avg       0.98      0.98      0.98       359\n",
      "\n",
      "Classification Report for category_b_12 with 100 selected features (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.40      0.39        30\n",
      "           1       0.48      0.47      0.47        30\n",
      "           2       0.69      0.67      0.68        30\n",
      "\n",
      "    accuracy                           0.51        90\n",
      "   macro avg       0.52      0.51      0.51        90\n",
      "weighted avg       0.52      0.51      0.51        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Assuming X and Y_category_b_12 are already defined\n",
    "X = data.values\n",
    "Y_category_b_12 = data_target['category_b_12'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_category_b_12 = label_encoder.fit_transform(Y_category_b_12)\n",
    "\n",
    "# Stratified split of the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_category_b_12, test_size=0.2, random_state=42, stratify=Y_category_b_12)\n",
    "\n",
    "# Step 1: Initialize XGBoost Classifier and RFE\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "rfe = RFE(estimator=xgb_clf, n_features_to_select=100)  # Choose top 100 features\n",
    "\n",
    "# Fit RFE to select top 100 features\n",
    "X_train_rfe = rfe.fit_transform(X_train, Y_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(500, 1000),  # Increased upper bound\n",
    "    'max_depth': randint(2, 8),          # Reduced depth\n",
    "    'learning_rate': uniform(0.01, 0.09),# Lower learning rates\n",
    "    'subsample': uniform(0.5, 0.5),      # Range from 0.5 to 1.0\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'min_child_weight': randint(5, 15),  # Increased minimum\n",
    "    'gamma': uniform(0, 5),              # Expanded range\n",
    "    'reg_alpha': uniform(0, 1),          # Added L1 regularization\n",
    "    'reg_lambda': uniform(0, 10),        # Added L2 regularization\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation on the selected features\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist, n_iter=50, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train_rfe, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from Randomized Search:\", random_search.best_params_)\n",
    "\n",
    "# Cross-validation on the training set with the best model\n",
    "cv_scores = cross_val_score(best_xgb, X_train_rfe, Y_train, cv=5, scoring='f1_weighted')\n",
    "print(\"Cross-validation F1 scores on training set:\", cv_scores)\n",
    "print(\"Average cross-validation F1 score on training set:\", np.mean(cv_scores))\n",
    "\n",
    "# Step 3: Fit the model on the training data for the final evaluation on test and train sets\n",
    "best_xgb.fit(X_train_rfe, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_xgb.predict(X_train_rfe)\n",
    "print(\"Classification Report for category_b_12 with 100 selected features (Train Set):\")\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "Y_test_pred = best_xgb.predict(X_test_rfe)\n",
    "print(\"Classification Report for category_b_12 with 100 selected features (Test Set):\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Assuming 'data' and 'data_target' are your feature matrix and target DataFrame respectively\n",
    "# X contains feature values\n",
    "X = data.values\n",
    "\n",
    "# Set the chunk size (number of target variables per part)\n",
    "chunk_size = 40\n",
    "\n",
    "# Loop through target columns in chunks\n",
    "for chunk_start in range(0, 241, chunk_size):\n",
    "    # Get the current chunk of target columns\n",
    "    chunk_end = min(chunk_start + chunk_size, 241)\n",
    "    target_chunk = data_target.columns[chunk_start:chunk_end]\n",
    "    \n",
    "    # Initialize list to store results for the current chunk\n",
    "    chunk_results = []\n",
    "\n",
    "    # Open a file to write classification reports for the current chunk\n",
    "    report_file_path = f\"/home/qiuaodon/Desktop/CRC_image/XGboost_results/Classification_Reports_100features_part_{chunk_start}_{chunk_end}.txt\"\n",
    "    with open(report_file_path, \"w\") as report_file:\n",
    "        # Iterate over each target column in the chunk\n",
    "        for target_col in target_chunk:\n",
    "            # Extract the target column for the current category\n",
    "            Y = data_target[target_col].values\n",
    "\n",
    "            # Encode labels if they are not numerical\n",
    "            label_encoder = LabelEncoder()\n",
    "            Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "            # Initialize the XGBoost Classifier\n",
    "            xgb_clf = xgb.XGBClassifier(\n",
    "                objective='multi:softmax', \n",
    "                random_state=42, \n",
    "                use_label_encoder=False,\n",
    "                eval_metric='mlogloss'\n",
    "            )\n",
    "\n",
    "            # Select top 100 features using RFE\n",
    "            rfe = RFE(estimator=xgb_clf, n_features_to_select=100)\n",
    "            X_selected = rfe.fit_transform(X, Y_encoded)\n",
    "            \n",
    "            # Split the data into training and testing sets with stratification\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                X_selected, Y_encoded, test_size=0.2, random_state=42, stratify=Y_encoded\n",
    "            )\n",
    "\n",
    "            # Define the hyperparameter distribution for XGBoost\n",
    "            param_dist = {\n",
    "                'n_estimators': randint(500, 1000),  # Increased upper bound\n",
    "                'max_depth': randint(2, 8),          # Reduced depth\n",
    "                'learning_rate': uniform(0.01, 0.09),# Lower learning rates\n",
    "                'subsample': uniform(0.5, 0.5),      # Range from 0.5 to 1.0\n",
    "                'colsample_bytree': uniform(0.5, 0.5),\n",
    "                'min_child_weight': randint(5, 15),  # Increased minimum\n",
    "                'gamma': uniform(0, 5),              # Expanded range\n",
    "                'reg_alpha': uniform(0, 1),          # Added L1 regularization\n",
    "                'reg_lambda': uniform(0, 10),        # Added L2 regularization\n",
    "            }\n",
    "\n",
    "            # Perform Randomized Search with cross-validation\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=xgb_clf,\n",
    "                param_distributions=param_dist,\n",
    "                n_iter=50, \n",
    "                cv=5, \n",
    "                scoring='f1_weighted', \n",
    "                n_jobs=-1, \n",
    "                verbose=1, \n",
    "                random_state=42\n",
    "            )\n",
    "            random_search.fit(X_train, Y_train)\n",
    "\n",
    "            # Get the best model from Randomized Search\n",
    "            best_xgb = random_search.best_estimator_\n",
    "            best_params = random_search.best_params_\n",
    "\n",
    "            # Fit the best model on the training data\n",
    "            best_xgb.fit(X_train, Y_train)\n",
    "\n",
    "            # Evaluate on the training set\n",
    "            Y_train_pred = best_xgb.predict(X_train)\n",
    "            train_report = classification_report(Y_train, Y_train_pred)\n",
    "\n",
    "            # Evaluate on the test set\n",
    "            Y_test_pred = best_xgb.predict(X_test)\n",
    "            test_report_dict = classification_report(Y_test, Y_test_pred, output_dict=True)\n",
    "            test_report = classification_report(Y_test, Y_test_pred)\n",
    "\n",
    "            # Write classification reports to the file\n",
    "            report_file.write(f\"Classification Report for {target_col} (Train Set):\\n\")\n",
    "            report_file.write(train_report)\n",
    "            report_file.write(\"\\n\")\n",
    "            report_file.write(f\"Classification Report for {target_col} (Test Set):\\n\")\n",
    "            report_file.write(test_report)\n",
    "            report_file.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "            # Calculate and store precision, recall, and accuracy\n",
    "            precision = precision_score(Y_test, Y_test_pred, average='weighted')\n",
    "            recall = recall_score(Y_test, Y_test_pred, average='weighted')\n",
    "            accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "\n",
    "            # Extract precision, recall, and f1-score specifically for class '1' (if it exists)\n",
    "            class_labels = label_encoder.classes_\n",
    "            class_indices = label_encoder.transform(class_labels)\n",
    "            class_1_index = np.where(class_indices == 1)[0]\n",
    "\n",
    "            if class_1_index.size > 0:\n",
    "                class_1_label = str(class_indices[class_1_index[0]])\n",
    "                class_1_metrics = test_report_dict.get(class_1_label, {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "            else:\n",
    "                class_1_metrics = {\"precision\": None, \"recall\": None, \"f1-score\": None}\n",
    "\n",
    "            chunk_results.append({\n",
    "                \"Target Variable\": target_col,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Class 1 Precision\": class_1_metrics[\"precision\"],\n",
    "                \"Class 1 Recall\": class_1_metrics[\"recall\"],\n",
    "                \"Class 1 F1-Score\": class_1_metrics[\"f1-score\"],\n",
    "                \"Best Hyperparameters\": best_params\n",
    "            })\n",
    "    \n",
    "    # Save the results for the current chunk to an Excel file\n",
    "    results_df = pd.DataFrame(chunk_results)\n",
    "    results_df.to_excel(\n",
    "        f\"/home/qiuaodon/Desktop/CRC_image/XGboost_results/Precision_Recall_Accuracy_100features_part_{chunk_start}_{chunk_end}.xlsx\",\n",
    "        index=False\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
