{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import seaborn as sns # statistical data visualization\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import RFE\n",
    "rf = RandomForestClassifier()\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('~/Desktop/project_data_new/embedding_768_TCGA_COAD.csv')\n",
    "# data.index = data['PatientID']\n",
    "# drop the last two columns\n",
    "# data = data.drop(data.columns[-2:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/Desktop/project_data_new/embedding_768_TCGA_COAD_90percent_sample.csv')\n",
    "data.index = data['PatientID']\n",
    "# drop the 'PatientID' column\n",
    "data = data.drop('PatientID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target = pd.read_csv('~/Desktop/project_data_new/target_768_avg_expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target.index = data_target['Unnamed: 0']\n",
    "data_target = data_target.drop(['Unnamed: 0'], axis = 1)\n",
    "# only keep the columns with category in the name\n",
    "data_target = data_target.loc[:, data_target.columns.str.contains('category')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.index.isin(data_target.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv('~/Desktop/project_data_new/embedding_768_TCGA_COAD_10percent_sample.csv')\n",
    "data_test.index = data_test['PatientID']\n",
    "# drop the 'PatientID' column\n",
    "data_test = data_test.drop('PatientID', axis=1)\n",
    "data_test = data_test[data_test.index.isin(data_target.index)]\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "# from scipy.stats import randint\n",
    "\n",
    "# # Initialize the Random Forest Classifier\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Extract the feature values from data\n",
    "# X = data.values\n",
    "\n",
    "# # Set the chunk size (number of target variables per part)\n",
    "# chunk_size = 40\n",
    "\n",
    "# # Loop through target columns in chunks\n",
    "# for chunk_start in range(120, 241, chunk_size):\n",
    "#     # Get the current chunk of target columns\n",
    "#     chunk_end = min(chunk_start + chunk_size, 241)\n",
    "#     target_chunk = data_target.columns[chunk_start:chunk_end]\n",
    "    \n",
    "#     # Initialize list to store results for the current chunk\n",
    "#     chunk_results = []\n",
    "\n",
    "#     # Open a file to write classification reports for the current chunk\n",
    "#     report_file_path = f\"/home/qiuaodon/Desktop/CRC_image/Best_100_features_Randomforest_90percents/Classification_Reports_100features_part_{chunk_start}_{chunk_end}.txt\"\n",
    "#     with open(report_file_path, \"w\") as report_file:\n",
    "#         #stratify \n",
    "#         for target_col in target_chunk:\n",
    "#             # Extract the target column for the current category\n",
    "#             Y = data_target[target_col].values\n",
    "\n",
    "#             # Select top 100 features using RFE\n",
    "#             rfe = RFE(rf, n_features_to_select=100)  # Choose top 100 features\n",
    "#             X_selected = rfe.fit_transform(X, Y)\n",
    "            \n",
    "#             # Split the data into training and testing sets\n",
    "#             #use stratify to cross validate\n",
    "            \n",
    "#             X_train, X_test, Y_train, Y_test = train_test_split(X_selected, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#             # Define the hyperparameter distribution\n",
    "#             param_dist = {\n",
    "#                 'n_estimators': randint(100, 200),\n",
    "#                 'max_depth': randint(20, 40),\n",
    "#                 'min_samples_split': randint(10, 20),\n",
    "#                 'min_samples_leaf': randint(1, 5),\n",
    "#                 'max_features': ['sqrt', 'log2']\n",
    "#             }\n",
    "\n",
    "#             # Perform Randomized Search with cross-validation\n",
    "#             random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=60, \n",
    "#                                                cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "#             random_search.fit(X_train, Y_train)\n",
    "\n",
    "#             # Get the best model from Randomized Search\n",
    "#             best_rf = random_search.best_estimator_\n",
    "#             best_params = random_search.best_params_\n",
    "\n",
    "#             # Fit the model on the training data for evaluation\n",
    "#             best_rf.fit(X_train, Y_train)\n",
    "\n",
    "#             # Evaluate on the training set\n",
    "#             Y_train_pred = best_rf.predict(X_train)\n",
    "#             train_report = classification_report(Y_train, Y_train_pred)\n",
    "            \n",
    "#             # Evaluate on the test set\n",
    "#             Y_test_pred = best_rf.predict(X_test)\n",
    "#             test_report = classification_report(Y_test, Y_test_pred, output_dict=True)\n",
    "\n",
    "#             # Write classification reports to the single file\n",
    "#             report_file.write(f\"Classification Report for {target_col} (Train Set):\\n\")\n",
    "#             report_file.write(train_report)\n",
    "#             report_file.write(\"\\n\")\n",
    "#             report_file.write(f\"Classification Report for {target_col} (Test Set):\\n\")\n",
    "#             report_file.write(classification_report(Y_test, Y_test_pred))\n",
    "#             report_file.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "#             # Calculate and store precision, recall, and accuracy\n",
    "#             precision = precision_score(Y_test, Y_test_pred, average='weighted')\n",
    "#             recall = recall_score(Y_test, Y_test_pred, average='weighted')\n",
    "#             accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "\n",
    "#             # Extract precision, recall, and f1-score specifically for class '1'\n",
    "#             class_1_metrics = test_report.get('1', {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "\n",
    "#             chunk_results.append({\n",
    "#                 \"Target Variable\": target_col,\n",
    "#                 \"Precision\": precision,\n",
    "#                 \"Recall\": recall,\n",
    "#                 \"Accuracy\": accuracy,\n",
    "#                 \"Class 1 Precision\": class_1_metrics[\"precision\"],\n",
    "#                 \"Class 1 Recall\": class_1_metrics[\"recall\"],\n",
    "#                 \"Class 1 F1-Score\": class_1_metrics[\"f1-score\"],\n",
    "#                 \"Best Hyperparameters\": best_params\n",
    "#             })\n",
    "    \n",
    "#     # Save the results for the current chunk to an Excel file\n",
    "#     results_df = pd.DataFrame(chunk_results)\n",
    "#     results_df.to_excel(f\"/home/qiuaodon/Desktop/CRC_image/Best_features_REF/Precision_Recall_Accuracy_100features_part_{chunk_start}_{chunk_end}.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407, 241)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Align the targets for training and unseen test sets\n",
    "data_target_train = data_target.loc[data.index]\n",
    "data_target_unseen = data_target.loc[data_test.index]\n",
    "data_target_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Extract the feature values from data\n",
    "X = data.values\n",
    "X_test_unseen = data_test.values\n",
    "# Set the chunk size (number of target variables per part)\n",
    "chunk_size = 40\n",
    "\n",
    "for chunk_start in range(200, 241, chunk_size):\n",
    "    # Get the current chunk of target columns\n",
    "    chunk_end = min(chunk_start + chunk_size, 241)\n",
    "    target_chunk = data_target.columns[chunk_start:chunk_end]\n",
    "    \n",
    "    # Initialize list to store results for the current chunk\n",
    "    chunk_results = []\n",
    "\n",
    "    # Open a file to write classification reports for the current chunk\n",
    "    report_file_path = f\"/home/qiuaodon/Desktop/CRC_image/Best_100_features_Randomforest_90percents/Classification_Reports_100features_part_{chunk_start}_{chunk_end}.txt\"\n",
    "    with open(report_file_path, \"w\") as report_file:\n",
    "        for target_col in target_chunk:\n",
    "            # Extract the target column for the current category from training/validation set\n",
    "            Y = data_target_train[target_col].values\n",
    "\n",
    "            # Select top 100 features using RFE\n",
    "            rfe = RFE(rf, n_features_to_select=100)  # Choose top 100 features\n",
    "            X_selected = rfe.fit_transform(X, Y)\n",
    "\n",
    "            # Split the remaining training/validation set into train and validation\n",
    "            X_train, X_val, Y_train, Y_val = train_test_split(X_selected, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "            # Define the hyperparameter distribution\n",
    "            param_dist = {\n",
    "                'n_estimators': randint(100, 200),\n",
    "                'max_depth': randint(20, 40),\n",
    "                'min_samples_split': randint(10, 20),\n",
    "                'min_samples_leaf': randint(1, 5),\n",
    "                'max_features': ['sqrt', 'log2']\n",
    "            }\n",
    "\n",
    "            # Perform Randomized Search with cross-validation\n",
    "            random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=60, \n",
    "                                               cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "            random_search.fit(X_train, Y_train)\n",
    "\n",
    "            # Get the best model from Randomized Search\n",
    "            best_rf = random_search.best_estimator_\n",
    "            best_params = random_search.best_params_\n",
    "\n",
    "            # Fit the model on the training data for evaluation\n",
    "            best_rf.fit(X_train, Y_train)\n",
    "\n",
    "            # Evaluate on the training set\n",
    "            Y_train_pred = best_rf.predict(X_train)\n",
    "            train_report = classification_report(Y_train, Y_train_pred)\n",
    "            \n",
    "            # Evaluate on the validation set\n",
    "            Y_val_pred = best_rf.predict(X_val)\n",
    "            val_report = classification_report(Y_val, Y_val_pred, output_dict=True)\n",
    "\n",
    "            # Test on the unseen test data\n",
    "            X_test_unseen_selected = rfe.transform(X_test_unseen)  # Transform unseen test set using fitted RFE\n",
    "            Y_test_unseen_col = data_target_unseen[target_col].values\n",
    "            Y_test_pred = best_rf.predict(X_test_unseen_selected)\n",
    "            unseen_test_report = classification_report(Y_test_unseen_col, Y_test_pred, output_dict=True)\n",
    "\n",
    "            # Write classification reports to the single file\n",
    "            report_file.write(f\"Classification Report for {target_col} (Train Set):\\n\")\n",
    "            report_file.write(train_report)\n",
    "            report_file.write(\"\\n\")\n",
    "            report_file.write(f\"Classification Report for {target_col} (Validation Set):\\n\")\n",
    "            report_file.write(classification_report(Y_val, Y_val_pred))\n",
    "            report_file.write(\"\\n\")\n",
    "            report_file.write(f\"Classification Report for {target_col} (Unseen Test Set):\\n\")\n",
    "            report_file.write(classification_report(Y_test_unseen_col, Y_test_pred))\n",
    "            report_file.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "            # Calculate and store metrics for validation set\n",
    "            precision = precision_score(Y_val, Y_val_pred, average='weighted')\n",
    "            recall = recall_score(Y_val, Y_val_pred, average='weighted')\n",
    "            accuracy = accuracy_score(Y_val, Y_val_pred)\n",
    "\n",
    "            # Extract validation Class 1 metrics\n",
    "            class_1_metrics = val_report.get('1', {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "\n",
    "            # Extract unseen test Class 1 metrics\n",
    "            class_1_unseen_metrics = unseen_test_report.get('1', {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "\n",
    "            # Calculate unseen test metrics\n",
    "            unseen_precision = precision_score(Y_test_unseen_col, Y_test_pred, average='weighted')\n",
    "            unseen_recall = recall_score(Y_test_unseen_col, Y_test_pred, average='weighted')\n",
    "            unseen_accuracy = accuracy_score(Y_test_unseen_col, Y_test_pred)\n",
    "\n",
    "            # Append results for the current target variable\n",
    "            chunk_results.append({\n",
    "                \"Target Variable\": target_col,\n",
    "                \"Validation Precision\": precision,\n",
    "                \"Validation Recall\": recall,\n",
    "                \"Validation Accuracy\": accuracy,\n",
    "                \"Class 1 Precision (Validation)\": class_1_metrics[\"precision\"],\n",
    "                \"Class 1 Recall (Validation)\": class_1_metrics[\"recall\"],\n",
    "                \"Class 1 F1-Score (Validation)\": class_1_metrics[\"f1-score\"],\n",
    "                \"Unseen Test Precision\": unseen_precision,\n",
    "                \"Unseen Test Recall\": unseen_recall,\n",
    "                \"Unseen Test Accuracy\": unseen_accuracy,\n",
    "                \"Class 1 Precision (Unseen Test)\": class_1_unseen_metrics[\"precision\"],\n",
    "                \"Class 1 Recall (Unseen Test)\": class_1_unseen_metrics[\"recall\"],\n",
    "                \"Class 1 F1-Score (Unseen Test)\": class_1_unseen_metrics[\"f1-score\"],\n",
    "                \"Best Hyperparameters\": best_params\n",
    "            })\n",
    "    \n",
    "    # Save the results for the current chunk to an Excel file\n",
    "    results_df = pd.DataFrame(chunk_results)\n",
    "    results_df.to_excel(f\"/home/qiuaodon/Desktop/CRC_image/Best_100_features_Randomforest_90percents/Precision_Recall_Accuracy_100features_part_{chunk_start}_{chunk_end}.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Variable</th>\n",
       "      <th>Validation Precision</th>\n",
       "      <th>Validation Recall</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Class 1 Precision (Validation)</th>\n",
       "      <th>Class 1 Recall (Validation)</th>\n",
       "      <th>Class 1 F1-Score (Validation)</th>\n",
       "      <th>Best Hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>category_stromal_34</td>\n",
       "      <td>0.47627</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>{'max_depth': 39, 'max_features': 'sqrt', 'min...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Target Variable  Validation Precision  Validation Recall  \\\n",
       "0  category_stromal_34               0.47627           0.487805   \n",
       "\n",
       "   Validation Accuracy  Class 1 Precision (Validation)  \\\n",
       "0             0.487805                        0.466667   \n",
       "\n",
       "   Class 1 Recall (Validation)  Class 1 F1-Score (Validation)  \\\n",
       "0                          0.5                       0.482759   \n",
       "\n",
       "                                Best Hyperparameters  \n",
       "0  {'max_depth': 39, 'max_features': 'sqrt', 'min...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Extract the feature values from data\n",
    "X = data.values\n",
    "# data_target only keep the index in data\n",
    "data_target = data_target[data_target.index.isin(data.index)]\n",
    "# Specify the single target variable\n",
    "target_col = \"category_stromal_34\"\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Set the path for the classification report file\n",
    "report_file_path = f\"/home/qiuaodon/Desktop/CRC_image/Best_100_features_Randomforest_90percents/Classification_Report_100features_{target_col}.txt\"\n",
    "\n",
    "# Extract the target column for the current category from training/validation set\n",
    "Y = data_target[target_col].values\n",
    "# Select top 100 features using RFE\n",
    "# rfe = RFE(rf, n_features_to_select=100)  # Choose top 100 features\n",
    "# X_selected = rfe.fit_transform(X, Y)\n",
    "rf.fit(X, Y)\n",
    "feature_importances = rf.feature_importances_\n",
    "top_100_indices = np.argsort(feature_importances)[-100:]  # Indices of top 100 features\n",
    "X_selected = X[:, top_100_indices] \n",
    "# Split the remaining training/validation set into train and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_selected, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "# Define the hyperparameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(200, 1000),\n",
    "    'max_depth': randint(30, 50),\n",
    "    'min_samples_split': randint(10, 28),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=200, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_rf = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Fit the model on the training data for evaluation\n",
    "best_rf.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_rf.predict(X_train)\n",
    "train_report = classification_report(Y_train, Y_train_pred)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "Y_val_pred = best_rf.predict(X_val)\n",
    "val_report = classification_report(Y_val, Y_val_pred, output_dict=True)\n",
    "\n",
    "\n",
    "# Write classification reports to a file\n",
    "with open(report_file_path, \"w\") as report_file:\n",
    "    report_file.write(f\"Classification Report for {target_col} (Train Set):\\n\")\n",
    "    report_file.write(train_report)\n",
    "    report_file.write(\"\\n\")\n",
    "    report_file.write(f\"Classification Report for {target_col} (Validation Set):\\n\")\n",
    "    report_file.write(classification_report(Y_val, Y_val_pred))\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "precision = precision_score(Y_val, Y_val_pred, average='weighted')\n",
    "recall = recall_score(Y_val, Y_val_pred, average='weighted')\n",
    "accuracy = accuracy_score(Y_val, Y_val_pred)\n",
    "\n",
    "# Extract validation Class 1 metrics\n",
    "class_1_metrics = val_report.get('1', {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "\n",
    "\n",
    "\n",
    "# Append results for the target variable\n",
    "results.append({\n",
    "    \"Target Variable\": target_col,\n",
    "    \"Validation Precision\": precision,\n",
    "    \"Validation Recall\": recall,\n",
    "    \"Validation Accuracy\": accuracy,\n",
    "    \"Class 1 Precision (Validation)\": class_1_metrics[\"precision\"],\n",
    "    \"Class 1 Recall (Validation)\": class_1_metrics[\"recall\"],\n",
    "    \"Class 1 F1-Score (Validation)\": class_1_metrics[\"f1-score\"],\n",
    "    \"Best Hyperparameters\": best_params\n",
    "})\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 40,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 10,\n",
       " 'n_estimators': 616}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Extract the feature values from data\n",
    "X = data.values\n",
    "X_test_unseen = data_test.values\n",
    "\n",
    "# Specify the single target variable\n",
    "target_col = \"category_stromal_34\"\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Set the path for the classification report file\n",
    "report_file_path = f\"/home/qiuaodon/Desktop/CRC_image/Best_100_features_Randomforest_90percents/Classification_Report_100features_{target_col}.txt\"\n",
    "\n",
    "# Extract the target column for the current category from training/validation set\n",
    "Y = data_target_train[target_col].values\n",
    "\n",
    "# Select top 100 features using RFE\n",
    "rfe = RFE(rf, n_features_to_select=100)  # Choose top 100 features\n",
    "X_selected = rfe.fit_transform(X, Y)\n",
    "\n",
    "# Split the remaining training/validation set into train and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_selected, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "# Define the hyperparameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 200),\n",
    "    'max_depth': randint(20, 40),\n",
    "    'min_samples_split': randint(10, 20),\n",
    "    'min_samples_leaf': randint(1, 5),\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=200, \n",
    "                                   cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Randomized Search\n",
    "best_rf = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Fit the model on the training data for evaluation\n",
    "best_rf.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_rf.predict(X_train)\n",
    "train_report = classification_report(Y_train, Y_train_pred)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "Y_val_pred = best_rf.predict(X_val)\n",
    "val_report = classification_report(Y_val, Y_val_pred, output_dict=True)\n",
    "\n",
    "# Test on the unseen test data\n",
    "X_test_unseen_selected = rfe.transform(X_test_unseen)  # Transform unseen test set using fitted RFE\n",
    "Y_test_unseen_col = data_target_unseen[target_col].values\n",
    "Y_test_pred = best_rf.predict(X_test_unseen_selected)\n",
    "unseen_test_report = classification_report(Y_test_unseen_col, Y_test_pred, output_dict=True)\n",
    "\n",
    "# Write classification reports to a file\n",
    "with open(report_file_path, \"w\") as report_file:\n",
    "    report_file.write(f\"Classification Report for {target_col} (Train Set):\\n\")\n",
    "    report_file.write(train_report)\n",
    "    report_file.write(\"\\n\")\n",
    "    report_file.write(f\"Classification Report for {target_col} (Validation Set):\\n\")\n",
    "    report_file.write(classification_report(Y_val, Y_val_pred))\n",
    "    report_file.write(\"\\n\")\n",
    "    report_file.write(f\"Classification Report for {target_col} (Unseen Test Set):\\n\")\n",
    "    report_file.write(classification_report(Y_test_unseen_col, Y_test_pred))\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "precision = precision_score(Y_val, Y_val_pred, average='weighted')\n",
    "recall = recall_score(Y_val, Y_val_pred, average='weighted')\n",
    "accuracy = accuracy_score(Y_val, Y_val_pred)\n",
    "\n",
    "# Extract validation Class 1 metrics\n",
    "class_1_metrics = val_report.get('1', {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "\n",
    "# Extract unseen test Class 1 metrics\n",
    "class_1_unseen_metrics = unseen_test_report.get('1', {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "\n",
    "# Calculate unseen test metrics\n",
    "unseen_precision = precision_score(Y_test_unseen_col, Y_test_pred, average='weighted')\n",
    "unseen_recall = recall_score(Y_test_unseen_col, Y_test_pred, average='weighted')\n",
    "unseen_accuracy = accuracy_score(Y_test_unseen_col, Y_test_pred)\n",
    "\n",
    "# Append results for the target variable\n",
    "results.append({\n",
    "    \"Target Variable\": target_col,\n",
    "    \"Validation Precision\": precision,\n",
    "    \"Validation Recall\": recall,\n",
    "    \"Validation Accuracy\": accuracy,\n",
    "    \"Class 1 Precision (Validation)\": class_1_metrics[\"precision\"],\n",
    "    \"Class 1 Recall (Validation)\": class_1_metrics[\"recall\"],\n",
    "    \"Class 1 F1-Score (Validation)\": class_1_metrics[\"f1-score\"],\n",
    "    \"Unseen Test Precision\": unseen_precision,\n",
    "    \"Unseen Test Recall\": unseen_recall,\n",
    "    \"Unseen Test Accuracy\": unseen_accuracy,\n",
    "    \"Class 1 Precision (Unseen Test)\": class_1_unseen_metrics[\"precision\"],\n",
    "    \"Class 1 Recall (Unseen Test)\": class_1_unseen_metrics[\"recall\"],\n",
    "    \"Class 1 F1-Score (Unseen Test)\": class_1_unseen_metrics[\"f1-score\"],\n",
    "    \"Best Hyperparameters\": best_params\n",
    "})\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Extract the feature values from data\n",
    "X = data.values\n",
    "X_test_unseen = data_test.values\n",
    "\n",
    "# Specify the single target variable\n",
    "target_col = \"category_stromal_34\"\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Set the path for the classification report file\n",
    "report_file_path = f\"/home/qiuaodon/Desktop/CRC_image/Best_100_features_Randomforest_90percents/Classification_Report_100features_{target_col}.txt\"\n",
    "\n",
    "# Extract the target column for the current category from training/validation set\n",
    "Y = data_target_train[target_col].values\n",
    "\n",
    "# Select top 100 features using RFE\n",
    "rfe = RFE(rf, n_features_to_select=100)  # Choose top 100 features\n",
    "X_selected = rfe.fit_transform(X, Y)\n",
    "\n",
    "# Split the remaining training/validation set into train and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_selected, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 107, 150, 200],\n",
    "    'max_depth': [20, 26, 30, 40],\n",
    "    'min_samples_split': [10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 5],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, \n",
    "                           scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the model on the training data for evaluation\n",
    "best_rf.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "Y_train_pred = best_rf.predict(X_train)\n",
    "train_report = classification_report(Y_train, Y_train_pred)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "Y_val_pred = best_rf.predict(X_val)\n",
    "val_report = classification_report(Y_val, Y_val_pred, output_dict=True)\n",
    "\n",
    "# Test on the unseen test data\n",
    "X_test_unseen_selected = rfe.transform(X_test_unseen)  # Transform unseen test set using fitted RFE\n",
    "Y_test_unseen_col = data_target_unseen[target_col].values\n",
    "Y_test_pred = best_rf.predict(X_test_unseen_selected)\n",
    "unseen_test_report = classification_report(Y_test_unseen_col, Y_test_pred, output_dict=True)\n",
    "\n",
    "# Write classification reports to a file\n",
    "with open(report_file_path, \"w\") as report_file:\n",
    "    report_file.write(f\"Classification Report for {target_col} (Train Set):\\n\")\n",
    "    report_file.write(train_report)\n",
    "    report_file.write(\"\\n\")\n",
    "    report_file.write(f\"Classification Report for {target_col} (Validation Set):\\n\")\n",
    "    report_file.write(classification_report(Y_val, Y_val_pred))\n",
    "    report_file.write(\"\\n\")\n",
    "    report_file.write(f\"Classification Report for {target_col} (Unseen Test Set):\\n\")\n",
    "    report_file.write(classification_report(Y_test_unseen_col, Y_test_pred))\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "precision = precision_score(Y_val, Y_val_pred, average='weighted')\n",
    "recall = recall_score(Y_val, Y_val_pred, average='weighted')\n",
    "accuracy = accuracy_score(Y_val, Y_val_pred)\n",
    "\n",
    "# Extract validation Class 1 metrics\n",
    "class_1_metrics = val_report.get('1', {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "\n",
    "# Extract unseen test Class 1 metrics\n",
    "class_1_unseen_metrics = unseen_test_report.get('1', {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "\n",
    "# Calculate unseen test metrics\n",
    "unseen_precision = precision_score(Y_test_unseen_col, Y_test_pred, average='weighted')\n",
    "unseen_recall = recall_score(Y_test_unseen_col, Y_test_pred, average='weighted')\n",
    "unseen_accuracy = accuracy_score(Y_test_unseen_col, Y_test_pred)\n",
    "\n",
    "# Append results for the target variable\n",
    "results.append({\n",
    "    \"Target Variable\": target_col,\n",
    "    \"Validation Precision\": precision,\n",
    "    \"Validation Recall\": recall,\n",
    "    \"Validation Accuracy\": accuracy,\n",
    "    \"Class 1 Precision (Validation)\": class_1_metrics[\"precision\"],\n",
    "    \"Class 1 Recall (Validation)\": class_1_metrics[\"recall\"],\n",
    "    \"Class 1 F1-Score (Validation)\": class_1_metrics[\"f1-score\"],\n",
    "    \"Unseen Test Precision\": unseen_precision,\n",
    "    \"Unseen Test Recall\": unseen_recall,\n",
    "    \"Unseen Test Accuracy\": unseen_accuracy,\n",
    "    \"Class 1 Precision (Unseen Test)\": class_1_unseen_metrics[\"precision\"],\n",
    "    \"Class 1 Recall (Unseen Test)\": class_1_unseen_metrics[\"recall\"],\n",
    "    \"Class 1 F1-Score (Unseen Test)\": class_1_unseen_metrics[\"f1-score\"],\n",
    "    \"Best Hyperparameters\": best_params\n",
    "})\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel(f\"/home/qiuaodon/Desktop/CRC_image/Best_100_features_Randomforest_90percents/Precision_Recall_Accuracy_100features_{target_col}.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Select top 100 features using RFE\u001b[39;00m\n\u001b[1;32m     27\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFE(rf, n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Choose top 100 features\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m X_selected \u001b[38;5;241m=\u001b[39m rfe\u001b[38;5;241m.\u001b[39mfit_transform(X, Y)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[1;32m     31\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_selected, Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_rfe.py:264\u001b[0m, in \u001b[0;36mRFE.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_rfe.py:311\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[0;32m--> 311\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m    314\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[1;32m    315\u001b[0m     estimator,\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[1;32m    317\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    318\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    492\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    493\u001b[0m )(\n\u001b[1;32m    494\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    495\u001b[0m         t,\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[1;32m    497\u001b[0m         X,\n\u001b[1;32m    498\u001b[0m         y,\n\u001b[1;32m    499\u001b[0m         sample_weight,\n\u001b[1;32m    500\u001b[0m         i,\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[1;32m    502\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    503\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[1;32m    504\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[1;32m    505\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    508\u001b[0m )\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    193\u001b[0m         X,\n\u001b[1;32m    194\u001b[0m         y,\n\u001b[1;32m    195\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight,\n\u001b[1;32m    196\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "# from scipy.stats import randint\n",
    "\n",
    "# # Initialize the Random Forest Classifier\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Extract the feature values from data\n",
    "# X = data.values\n",
    "\n",
    "# # Initialize lists to store precision, recall, and accuracy for each target variable\n",
    "# results = []\n",
    "\n",
    "# # Open a single file to write all classification reports\n",
    "# with open(\"/home/qiuaodon/Desktop/CRC_image/Best_features_REF/Classification_Reports_100features.txt\", \"w\") as report_file:\n",
    "    \n",
    "#     # Loop through the first 120 target columns in data_target\n",
    "#     for i, target_col in enumerate(data_target.columns[:241]):\n",
    "#         # Extract the target column for the current category\n",
    "#         Y = data_target[target_col].values\n",
    "\n",
    "#         # Select top 100 features using RFE\n",
    "#         rfe = RFE(rf, n_features_to_select=100)  # Choose top 100 features\n",
    "#         X_selected = rfe.fit_transform(X, Y)\n",
    "        \n",
    "#         # Split the data into training and testing sets\n",
    "#         X_train, X_test, Y_train, Y_test = train_test_split(X_selected, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#         # Define the hyperparameter distribution (use distributions for random search)\n",
    "#         param_dist = {\n",
    "#             'n_estimators': randint(100, 200),\n",
    "#             'max_depth': randint(20, 40),\n",
    "#             'min_samples_split': randint(10, 20),\n",
    "#             'min_samples_leaf': randint(1, 5),\n",
    "#             'max_features': ['sqrt', 'log2']\n",
    "#         }\n",
    "\n",
    "#         # Perform Randomized Search with cross-validation\n",
    "#         random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=60, \n",
    "#                                            cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=42)\n",
    "#         random_search.fit(X_train, Y_train)\n",
    "\n",
    "#         # Get the best model from Randomized Search\n",
    "#         best_rf = random_search.best_estimator_\n",
    "#         best_params = random_search.best_params_\n",
    "\n",
    "#         # Fit the model on the training data for the final evaluation on train and test sets\n",
    "#         best_rf.fit(X_train, Y_train)\n",
    "\n",
    "#         # Evaluate on the training set\n",
    "#         Y_train_pred = best_rf.predict(X_train)\n",
    "#         train_report = classification_report(Y_train, Y_train_pred)\n",
    "        \n",
    "#         # Evaluate on the test set\n",
    "#         Y_test_pred = best_rf.predict(X_test)\n",
    "#         test_report = classification_report(Y_test, Y_test_pred, output_dict=True)\n",
    "\n",
    "#         # Write classification reports to the single file\n",
    "#         report_file.write(f\"Classification Report for {target_col} (Train Set):\\n\")\n",
    "#         report_file.write(train_report)\n",
    "#         report_file.write(\"\\n\")\n",
    "#         report_file.write(f\"Classification Report for {target_col} (Test Set):\\n\")\n",
    "#         report_file.write(classification_report(Y_test, Y_test_pred))\n",
    "#         report_file.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "#         # Calculate and store precision, recall, and accuracy\n",
    "#         precision = precision_score(Y_test, Y_test_pred, average='weighted')\n",
    "#         recall = recall_score(Y_test, Y_test_pred, average='weighted')\n",
    "#         accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "\n",
    "#         # Extract precision, recall, and f1-score specifically for class '1'\n",
    "#         class_1_metrics = test_report.get('1', {\"precision\": None, \"recall\": None, \"f1-score\": None})\n",
    "\n",
    "#         results.append({\n",
    "#             \"Target Variable\": target_col,\n",
    "#             \"Precision\": precision,\n",
    "#             \"Recall\": recall,\n",
    "#             \"Accuracy\": accuracy,\n",
    "#             \"Class 1 Precision\": class_1_metrics[\"precision\"],\n",
    "#             \"Class 1 Recall\": class_1_metrics[\"recall\"],\n",
    "#             \"Class 1 F1-Score\": class_1_metrics[\"f1-score\"],\n",
    "#             \"Best Hyperparameters\": best_params\n",
    "#         })\n",
    "\n",
    "# # Save the results to an Excel file\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_excel(\"/home/qiuaodon/Desktop/CRC_image/Best_features_REF/Precision_Recall_Accuracy_100features.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Variable</th>\n",
       "      <th>Validation Precision</th>\n",
       "      <th>Validation Recall</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Class 1 Precision (Validation)</th>\n",
       "      <th>Class 1 Recall (Validation)</th>\n",
       "      <th>Class 1 F1-Score (Validation)</th>\n",
       "      <th>Unseen Test Precision</th>\n",
       "      <th>Unseen Test Recall</th>\n",
       "      <th>Unseen Test Accuracy</th>\n",
       "      <th>Class 1 Precision (Unseen Test)</th>\n",
       "      <th>Class 1 Recall (Unseen Test)</th>\n",
       "      <th>Class 1 F1-Score (Unseen Test)</th>\n",
       "      <th>Best Hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>category_tnk_1</td>\n",
       "      <td>0.476098</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.371849</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>{'max_depth': 25, 'max_features': 'log2', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>category_tnk_2</td>\n",
       "      <td>0.514563</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.543515</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>{'max_depth': 23, 'max_features': 'sqrt', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>category_tnk_3</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.180934</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>{'max_depth': 37, 'max_features': 'log2', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>category_tnk_4</td>\n",
       "      <td>0.560694</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.588409</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>{'max_depth': 28, 'max_features': 'sqrt', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>category_tnk_5</td>\n",
       "      <td>0.546459</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.353672</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>{'max_depth': 36, 'max_features': 'sqrt', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>category_stromal_10</td>\n",
       "      <td>0.456924</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.480866</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>{'max_depth': 30, 'max_features': 'log2', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>category_stromal_11</td>\n",
       "      <td>0.501768</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.367565</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>{'max_depth': 39, 'max_features': 'sqrt', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>category_stromal_12</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.379085</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>{'max_depth': 31, 'max_features': 'log2', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>category_stromal_13</td>\n",
       "      <td>0.565932</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.391484</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>{'max_depth': 22, 'max_features': 'sqrt', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>category_stromal_14</td>\n",
       "      <td>0.428033</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.529832</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>{'max_depth': 35, 'max_features': 'sqrt', 'min...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Target Variable  Validation Precision  Validation Recall  \\\n",
       "0         category_tnk_1              0.476098           0.487805   \n",
       "1         category_tnk_2              0.514563           0.512195   \n",
       "2         category_tnk_3              0.483284           0.487805   \n",
       "3         category_tnk_4              0.560694           0.548780   \n",
       "4         category_tnk_5              0.546459           0.548780   \n",
       "..                   ...                   ...                ...   \n",
       "236  category_stromal_10              0.456924           0.451220   \n",
       "237  category_stromal_11              0.501768           0.500000   \n",
       "238  category_stromal_12              0.426071           0.426829   \n",
       "239  category_stromal_13              0.565932           0.560976   \n",
       "240  category_stromal_14              0.428033           0.426829   \n",
       "\n",
       "     Validation Accuracy  Class 1 Precision (Validation)  \\\n",
       "0               0.487805                        0.535714   \n",
       "1               0.512195                        0.483871   \n",
       "2               0.487805                        0.448276   \n",
       "3               0.548780                        0.650000   \n",
       "4               0.548780                        0.560000   \n",
       "..                   ...                             ...   \n",
       "236             0.451220                        0.451613   \n",
       "237             0.500000                        0.523810   \n",
       "238             0.426829                        0.379310   \n",
       "239             0.560976                        0.636364   \n",
       "240             0.426829                        0.400000   \n",
       "\n",
       "     Class 1 Recall (Validation)  Class 1 F1-Score (Validation)  \\\n",
       "0                       0.555556                       0.545455   \n",
       "1                       0.555556                       0.517241   \n",
       "2                       0.464286                       0.456140   \n",
       "3                       0.500000                       0.565217   \n",
       "4                       0.518519                       0.538462   \n",
       "..                           ...                            ...   \n",
       "236                     0.500000                       0.474576   \n",
       "237                     0.407407                       0.458333   \n",
       "238                     0.407407                       0.392857   \n",
       "239                     0.500000                       0.560000   \n",
       "240                     0.370370                       0.384615   \n",
       "\n",
       "     Unseen Test Precision  Unseen Test Recall  Unseen Test Accuracy  \\\n",
       "0                 0.371849            0.357143              0.357143   \n",
       "1                 0.543515            0.404762              0.404762   \n",
       "2                 0.180934            0.166667              0.166667   \n",
       "3                 0.588409            0.547619              0.547619   \n",
       "4                 0.353672            0.309524              0.309524   \n",
       "..                     ...                 ...                   ...   \n",
       "236               0.480866            0.428571              0.428571   \n",
       "237               0.367565            0.333333              0.333333   \n",
       "238               0.379085            0.404762              0.404762   \n",
       "239               0.391484            0.357143              0.357143   \n",
       "240               0.529832            0.523810              0.523810   \n",
       "\n",
       "     Class 1 Precision (Unseen Test)  Class 1 Recall (Unseen Test)  \\\n",
       "0                           0.294118                      0.357143   \n",
       "1                           0.375000                      0.400000   \n",
       "2                           0.125000                      0.181818   \n",
       "3                           0.733333                      0.523810   \n",
       "4                           0.266667                      0.285714   \n",
       "..                               ...                           ...   \n",
       "236                         0.315789                      0.600000   \n",
       "237                         0.363636                      0.266667   \n",
       "238                         0.500000                      0.500000   \n",
       "239                         0.250000                      0.363636   \n",
       "240                         0.647059                      0.687500   \n",
       "\n",
       "     Class 1 F1-Score (Unseen Test)  \\\n",
       "0                          0.322581   \n",
       "1                          0.387097   \n",
       "2                          0.148148   \n",
       "3                          0.611111   \n",
       "4                          0.275862   \n",
       "..                              ...   \n",
       "236                        0.413793   \n",
       "237                        0.307692   \n",
       "238                        0.500000   \n",
       "239                        0.296296   \n",
       "240                        0.666667   \n",
       "\n",
       "                                  Best Hyperparameters  \n",
       "0    {'max_depth': 25, 'max_features': 'log2', 'min...  \n",
       "1    {'max_depth': 23, 'max_features': 'sqrt', 'min...  \n",
       "2    {'max_depth': 37, 'max_features': 'log2', 'min...  \n",
       "3    {'max_depth': 28, 'max_features': 'sqrt', 'min...  \n",
       "4    {'max_depth': 36, 'max_features': 'sqrt', 'min...  \n",
       "..                                                 ...  \n",
       "236  {'max_depth': 30, 'max_features': 'log2', 'min...  \n",
       "237  {'max_depth': 39, 'max_features': 'sqrt', 'min...  \n",
       "238  {'max_depth': 31, 'max_features': 'log2', 'min...  \n",
       "239  {'max_depth': 22, 'max_features': 'sqrt', 'min...  \n",
       "240  {'max_depth': 35, 'max_features': 'sqrt', 'min...  \n",
       "\n",
       "[241 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Specify the directory where your files are located\n",
    "file_path_pattern = '/home/qiuaodon/Desktop/CRC_image/Best_100_features_Randomforest_90percents/Precision_Recall_Accuracy_100features_part_*.xlsx'\n",
    "\n",
    "# Use glob to find all matching files\n",
    "all_files = glob.glob(file_path_pattern)\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "data_frames = []\n",
    "\n",
    "# Loop through each file and append its DataFrame to the list\n",
    "for file in all_files:\n",
    "    df = pd.read_excel(file)\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new Excel file\n",
    "combined_df.to_excel('/home/qiuaodon/Desktop/CRC_image/Best_100_features_Randomforest_90percents/top_100features_Randomforest_241_targets_90train.xlsx', index=False)\n",
    "combined_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
